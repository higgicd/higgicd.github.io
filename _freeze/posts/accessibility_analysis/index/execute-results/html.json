{
  "hash": "bb6ca9ca653c8993568c8af2bb85b8f5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Accessibility Analysis in Toronto\"\ndate: \"2025-05-21\"\ncategories: [code, analysis]\nimage: \"banner.jpg\"\nbibliography: references.bib\ncode-annotations: hover\n---\n\n\n\nOver this past academic year, we were lucky to host Dr. Rafael Pereira as a Bousfield Distinguished Visiting Professor in the Department of Geography and Planning. Rafa earned his PhD at Oxford and has published dozens of great academic papers. He is also the lead of the data science team at the Institute for Applied Economic Research (Ipea) which has published the {r5r} [package](https://github.com/ipeaGIT/r5r), which enables rapid realistic routing on multimodal transport networks by connecting R with the open source [R5 Routing Engine](https://github.com/conveyal/r5) from Conveyal. Beyond his Bousfield Lecture on [Advancing Urban Accessibility for Inclusive Cities](https://www.geography.utoronto.ca/news/bousfield-lecture-presented-rafael-pereira), he also led a crash-course workshop on [Urban Accessibility with R](https://ipeagit.github.io/access_workshop_toronto_2025/) where students and practitioners in attendance got some hands-on experience using R for accessibility analysis with some data for Brazil.\n\nBetween the workshop, my teaching of courses like GGRC30 Advanced GIS and JPG1400 Advanced Quantitative Methods where students are increasingly conducting accessibility analyses and often needing some guidance, and the joking by Steven Farber that I should be local tech support for {r5r}, I've put together a collection of the first steps I undertake in nearly every analysis of accessibility in the City of Toronto and larger region. Together, these serve as a nice little introductory vignette to the topic in the local context.\n\n# What is Accessibility?\n\nTo jump right into the topic, transportation accessibility reflects the ease with which individuals can reach destinations using the transportation network. There are many different types of accessibility measures (see @geurs2004's review of the infrastructure-, place-, person- and utility-based approaches or @wu2020's *Unifying Access* paper). This post focuses on place-based accessibility - the potential to reach destinations from an origin place using the transportation network. To keep this post manageable, I am assuming some general knowledge of place-based accessibility and how {r5r} works. If you need a refresher, see the [Introduction to Urban Accessibility](https://ipeagit.github.io/intro_access_book/) book by Rafa and his Ipea team. Of my own work, see Higgins et al. -@higgins2022 and Higgins -@higgins2019. \n\nBut briefly, a place-based accessibility measure takes the general form:\n\n$$\nA_i = \\sum_{j=1}^J O_j \\times f(t_{ij})\n$$\n\n\\noindent where the accessibility for place $i$, typically represented by a polygon, is the sum of opportunities $O$ at the destinations $j$ weighted by some function $f$ of the travel time required to reach them ($t_{ij}$). Place-based measures often utilize a cumulative (e.g. a 45-min travel time cut-off) or gravity-type (continuously declining) impedance function to account for the fact that there are costs associated with travel that make destinations farther away generally less desirable. \n\nAccessibility analyses have a lot of uses, and even Statistics Canada has conducted some across the country through their [Spatial Access Measures](https://www150.statcan.gc.ca/n1/pub/27-26-0001/272600012023001-eng.htm) database. But assuming you want to do this yourself, let's jump into how we can get set up for calculating accessibility in the Toronto region.\n\n# Setup\n\nBecause the {r5r} package uses Java, the first thing we have to do is allocate some memory for Java to operate in. Here I will allocate 8gb of memory, which should be enough for R5 to work with a network for the region:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(java.parameters = \"-Xmx8G\")\n```\n:::\n\n\n\nNext, load the packages we need\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tidyverse\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(janitor)\n\n# spatial\nlibrary(sf)\nlibrary(tmap)\n\n# data and analysis\nlibrary(cancensus)\nlibrary(osmextract)\nlibrary(TTS2016R)\nlibrary(calendR)\n\n# routing and transit\nlibrary(r5r)\nlibrary(accessibility)\nlibrary(tidytransit)\n```\n:::\n\n\n\nFinally, we will set up file path helpers to point to where we will keep our cache, R5 network graph, and travel time matrix:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncache_path <- fs::dir_create(\"./cache\")\nr5_graph_path <- fs::dir_create(\"./r5_graph\")\nttm_path <- fs::dir_create(\"./ttm\")\n```\n:::\n\n\n\n# Data Collection\n## Origin Places\n\nPlace-based measures of access require information on the locations of the origin places $i$. One popular option for origin and destination places is Census zones, such as Dissemination Areas (DAs) or Census Tracts (CTs). These can be obtained using the great {cancensus} package. Please see the vignettes from the [package documentation](https://mountainmath.github.io/cancensus/) to learn more about getting a free API key, finding census datasets, regions, and vectors. First, we need to provide an API key:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_cancensus_api_key(\"<your API key>\", install = TRUE)\n```\n:::\n\n\n\nI will also set the cache directory for this project to avoid multiple downloads:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_cancensus_cache_path(cache_path)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n./cache\n```\n\n\n:::\n:::\n\n\n\nWith this set, we can now get some data for CTs for the Toronto and Oshawa CMAs that traditionally make up the Greater Toronto Area:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_data_ct <- get_census(\n    dataset = 'CA16', # <1>\n    regions = list(CMA = c(\"35532\", \"35535\")), # <2>\n    level = 'CT', # <3>\n    geo_format = \"sf\", # <4>\n    use_cache = TRUE) |> \n  janitor::clean_names() |> # <5>\n  st_transform(crs = 26918) |> # <6>\n  mutate(population_density = population / shape_area) # <7>\n```\n:::\n\n\n\n1.  Use the 2016 Census\n2.  Specify CMAs as the province code for Ontario (\"35\") and the CMA codes for Oshawa (\"532\") and Toronto (\"535\")\n3.  Get Census Tracts\n4.  Tell {cancensus} we want the CT geometries in {sf} format\n5.  Often Census data comes with capital letters and spaces in the names, so use {janitor} to clean the names up\n6.  Transform the {sf} geometries to the NAD 1983 Zone 17n projection for the region ([EPSG code `26917`](https://epsg.io/26917))\n7.  Mutate a population density (in people per $km^2$) column\n\nLet's see what this data looks like by mapping our `population_density` variable using {tmap}:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntm_shape(census_data_ct) + \n  tm_fill(\n    fill = \"population_density\", # <1>\n    fill.scale = tm_scale_intervals( # <2>\n      n = 10, # <3>\n      style = \"jenks\", # <4>\n      values = \"viridis\" # <5>\n      ), \n    fill.legend = tm_legend(\n      title = \"population density \\n(people per km\\U00B2)\", # <6>\n      frame = FALSE # <7>\n      )\n    )\n```\n\n::: {.cell-output-display}\n![Population Densities (2016) for CTs in the Greater Toronto Area](index_files/figure-html/fig-ct_population_density-1.png){#fig-ct_population_density width=672}\n:::\n:::\n\n\n\n1.  tell {tmap} we want to map the `population_density` variable\n2.  use an interval-based classification scheme\n3.  with 10 breaks in the distribution\n4.  use the Jenks algorithm to set values for the 10 breaks\n5.  use the viridis colour scheme\n6.  set the legend title, including a line break using `\\n` and the unicode character `\\U00B2` for a superscript of 2\n7.  turn off the legend frame\n\nWithin the Toronto region, another option for origin and destination places is the traffic analysis zones (TAZs) associated with the [Transportation Tomorrow Survey](https://dmg.utoronto.ca/tts-introduction/) (TTS). This survey covers the larger Greater Golden Horseshoe area with zones roughly similar in size to CTs. A recent package called {TTS2016R} has gathered these zones for easy use in R, and I will filter them down to just the zones in the Toronto and Oshawa CMAs:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntts_tazs <- TTS2016R::ggh_taz |> \n  janitor::clean_names() |>\n  sf::st_transform(crs = 26917) |> \n  filter(cmauid %in% c(\"532\", \"535\")) |> \n  mutate(\n    workers_density = workers / area,\n    jobs_density = jobs / area)\n```\n:::\n\n\n\nThis survey covers the larger Greater Golden Horseshoe area with zones roughly similar in size to CTs and the {TTS2016R} package focuses on worker and job counts. Here's a map of the density of the working population:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntm_shape(tts_tazs) + \n  tm_fill(\n    fill = \"workers_density\", \n    fill.scale = tm_scale_intervals(\n      n = 10, #\n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"worker density \\n(people per km\\U00B2)\", \n      frame = FALSE \n      )\n    )\n```\n\n::: {.cell-output-display}\n![Worker Densities (2016) for TAZs in the Greater Toronto Area](index_files/figure-html/fig-taz_workers-1.png){#fig-taz_workers width=672}\n:::\n:::\n\n\n\n## Destination Opportunities\n\nWe also need some destination places $j$, as well as some representation of the opportunities $O$ at the destinations. There are many different types of destination opportunities that can be considered for accessibility analysis. A popular option is employment counts at the destination zones. This data can be a bit hard to track down, but it is available for DAs from the 2016 Census via a custom extract of the Employed Labour Force by Place of Work hosted on Borealis [here](https://doi.org/10.5683/SP2/NTZFMT). This data comes as an Excel spreadsheet and will require some prep outside of R. \n\nThe TTS also captures employment counts at the destination TAZs and the survey data is available now for 2022 via the Data Management Group. Helpfully, the {TTS2016R} package offers job counts from the 2016 TTS at the TAZ level - here's a map of employment counts for the TAZs that make up the Toronto and Oshawa CMAs:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntm_shape(tts_tazs) + \n  tm_fill(\n    fill = \"jobs\", \n    fill.scale = tm_scale_intervals(\n      n = 10, #\n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"employment count\", \n      frame = FALSE \n      )\n    )\n```\n\n::: {.cell-output-display}\n![Employment Counts (2016) for TAZs in the Greater Toronto Area](index_files/figure-html/fig-taz_employment-1.png){#fig-taz_employment width=672}\n:::\n:::\n\n\n\nBeyond employment counts, other popular destinations include point of interest (POI) data for things like hospitals, grocery stores, etc. Often in these cases, the \"opportunity\" weight for a POI will be equal to one so that the accessibility analysis counts the number of a given POI type accessible from an origin place. One source for this for University students is DMTI's Enhanced Points of Interest database, which can be found on the [Scholar's GeoPortal](https://geo1.scholarsportal.info) after logging-in with your university credentials. This database can be filtered by the [North American Industry Classification System](https://www23.statcan.gc.ca/imdb/p3VD.pl?Function=getVD&TVD=1369825) (NAICS) codes for the POIs. We used the DMTI data in @yu2024.\n\nAlternatively, POI data collected from sources like Microsoft and Meta is also now available as an open data product through [Overture Maps](https://overturemaps.org) and can accessed via new R packages such as the {overtureR} [package](https://arthurgailes.github.io/overtureR/). Other POIs like parks and schools can be found from municipal (e.g. [Toronto](https://open.toronto.ca)), provincial ([Ontario](https://data.ontario.ca)), and federal open data portals. For example, Statistics Canada has been collecting a range of open data products through their [Linkable Open Data Environment](https://www.statcan.gc.ca/en/lode/databases) project.\n\n## OpenStreetMap File\n\nWith the origin places and destination opportunities collected, next you need the core components that R5 and {r5r} need to create a routable network. The first of these is a street network from OpenStreetMap (OSM). The way I do this is through the {osmextract} package, which allows you to search by a place name. Pre-defined OSM extracts can be found for major places around the world. These are great because they are relatively small and won't cause an error associated with the maximum study area size of 975,000 $km^2$ in R5. Outside of R, you can also find metro area extracts from [Interline](https://app.interline.io/osm_extracts/interactive_view) (API key required).\n\nHowever, if a place extract does not exist, you might have to move up to the next level of geography, which, in the Canadian case, could entail downloading gigabytes of OSM data for an entire province. This is a much trickier situation and requires the use of tools like {rosmosis} (the easier but slower tool, see [here](https://dhersz.github.io/rosmosis/)) or {rosmium} (the faster but harder to install and use tool, see [here](https://ipeagit.github.io/rosmium/)) to clip the OSM network to a bounding box in R.\n\nIn the Toronto case, there are two good extract options. The first is a \\~70mb extract for the GTA available from \"bbbike\", but it really only covers the City of Toronto.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nosmextract::oe_match(\"Toronto\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNo exact match found for place = Toronto and provider = geofabrik. Best match is Morocco. \nChecking the other providers.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nAn exact string match was found using provider = bbbike.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$url\n[1] \"https://download.bbbike.org/osm/bbbike/Toronto/Toronto.osm.pbf\"\n\n$file_size\n[1] 72915302\n```\n\n\n:::\n:::\n\n\n\nFor working in the GTA and/or to include cities like Hamilton, Waterloo, etc., there is also a great extract for the \"Golden Horseshoe\" available from \"openstreetmap_fr\":\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nosmextract::oe_match(\"Golden Horseshoe\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNo exact match found for place = Golden Horseshoe and provider = geofabrik. Best match is Centro-Oeste. \nChecking the other providers.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nAn exact string match was found using provider = openstreetmap_fr.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$url\n[1] \"http://download.openstreetmap.fr/extracts/north-america/canada/ontario/golden_horseshoe-latest.osm.pbf\"\n\n$file_size\n[1] 156983172\n```\n\n\n:::\n:::\n\n\n\nLet's download the OSM extract for the Golden Horseshoe to our `r5_graph_path` by providing the \"openstreetmap_fr\" URL to the `oe_download()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nosmextract::oe_download(\n  file_url = \"http://download.openstreetmap.fr/extracts/north-america/canada/ontario/golden_horseshoe-latest.osm.pbf\",\n  provider = \"openstreetmap_fr\",\n  download_directory = r5_graph_path\n)\n```\n:::\n\n\n\nWe can read in the lines layer of this OSM `.pbf` file as a simple features {sf} object using `oe_read()`, which translates the `.pbf` into a geopackage (`.gpkg`) in our `r5_graph_path` folder:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nosm_lines_sf <- osmextract::oe_read(\n  file_path = fs::path(r5_graph_path, \"openstreetmap_fr_golden_horseshoe-latest.osm.pbf\"),\n  layer = \"lines\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe corresponding gpkg file was already detected. Skip vectortranslate operations.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `lines' from data source \n  `/Users/chris/Library/CloudStorage/OneDrive-UniversityofToronto/GitHub/higgicd.github.io/posts/accessibility_analysis/r5_graph/openstreetmap_fr_golden_horseshoe-latest.gpkg' \n  using driver `GPKG'\nSimple feature collection with 939997 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -82.36947 ymin: 42.51469 xmax: -76.16424 ymax: 44.98971\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n:::\n\n\n\nWhen plotting the major roadways on the map, we can see that this extract covers this part of Southern Ontario:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntm_shape(osm_lines_sf |> \n           filter(highway %in% c(\"motorway\", \"primary\", \"secondary\"))) +\n  tm_lines(\n    col = \"highway\", \n    col.scale = tm_scale_categorical(values = \"plasma\"),\n    col.legend = tm_legend(\n      title = \"street type\", \n      frame = FALSE \n      )\n    )\n```\n\n::: {.cell-output-display}\n![Major Street Types from the Golden Horseshoe OSM Extract](index_files/figure-html/fig-streets-1.png){#fig-streets width=672}\n:::\n:::\n\n\n\n## GTFS Data\n\nThe second key input is General Transit Feed Specification (GTFS) static schedule files to enable transit routing. These can be found from a variety of sources, including municipal or agency open data portals. However, these are often only the most current schedule files. To better align with our 2016 place data, we can download GTFS files from other archival sources including:\n\n-   [transitfeeds](https://transitfeeds.com) which is great for historical data but is no longer current\n-   [mobility database](https://mobilitydatabase.org) which is replacing transitfeeds (I have not used yet)\n-   [transit.land](https://www.transit.land) a newer source, but it does require an API key to download historical files (free for hobbyist and academic use, see bottom of the [pricing page](https://www.transit.land/plans-pricing/))\n-   gtfs exchange was one of the earliest archives, you could still try to access it through the [Internet Archive](https://archive.org)\n\nFor Toronto, the fragmentation of the region means there are a number of different transit providers (if you think Toronto is annoying, see Montreal!). This makes things tricky in that first you have to track them all down (the browse by region in transitfeeds and transit.land is good for this) but also - and this is critical - they have to all have some service calendar days that align with each other. \n\nIn the Toronto case, you can reliably get GTFS feeds for the major providers across the region back to about fall 2016 from transitfeeds, and I have collected the download URLs for feeds around September 2016 into this named list:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngtfs_list <- list(\n  \"brampton\" = \"https://transitfeeds.com/p/brampton-transit/35/20160818/download\",\n  \"burlington\" = \"https://transitfeeds.com/p/burlington-transit/294/20160906/download\",\n  \"durham\" = \"https://transitfeeds.com/p/durham-region-transit/642/20160824/download\",\n  \"go\" = \"https://transitfeeds.com/p/go-transit/32/20160906/download\",\n  \"mississauga\" = \"https://transitfeeds.com/p/miway/641/20160907/download\",\n  \"oakville\" = \"https://transitfeeds.com/p/oakville-transit/615/20160901/download\",\n  \"toronto\" = \"https://transitfeeds.com/p/ttc/33/20160829/download\",\n  \"york\" = \"https://transitfeeds.com/p/york-regional-transit/34/20160904/download\"\n)\n```\n:::\n\n\n\nNext, I will pass this list to an `iwalk()` function from {purrr} for iteration with an index. The function iterates the `req_perform()` function for downloading files from the {httr} package and saves the output to the `r5_graph_path` folder:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngtfs_list |> # <1>\n  purrr::iwalk(~ httr2::req_perform( # <2>\n    req = httr2::request(.x) |> # <3>\n      httr2::req_cache(path = cache_path), # <4>\n    path = fs::path(r5_graph_path, paste0(.y, \".zip\"))) # <5>\n  )\n```\n:::\n\n\n\n1.  pass the `gtfs_list` into the pipeline\n2.  call the `req_perform()` function inside `iwalk()`\n3.  create a request object using the elements of the list (the URLs) represented as `.x`\n4.  I have added a cache option to cache the files to the `cache_path`\n5.  name the output `.zip` files using the list index (the names) represented as `.y` and save to the `r5_graph_path`\n\nWe should now have our GTFS files and the OSM network in our `r5_graph_path` folder:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfs::dir_tree(r5_graph_path, glob = \"*.zip\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n./r5_graph\n├── brampton.zip\n├── burlington.zip\n├── durham.zip\n├── go.zip\n├── mississauga.zip\n├── oakville.zip\n├── toronto.zip\n└── york.zip\n```\n\n\n:::\n:::\n\n\n\n\nAs a last step, let's verify that our service calendars do overlap. If they don't, the departure datetime that we pick for transit routing will omit any services that don't have scheduled operations on that day. To facilitate this, I have created a function called `check_gtfs_overlap()` that reads in a folder of GTFS files and returns their service calendars:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncheck_gtfs_overlap <- function(gtfs_folder) {\n  \n  # get a list of gtfs zip files in the gtfs directory\n  gtfs_zip_list <- fs::dir_ls(gtfs_folder, regexp = \"*.zip\")\n  \n  # get provider names from the file list\n  gtfs_zip_names <- fs::path_file(gtfs_zip_list) |> path_ext_remove()\n  \n  # read in gtfs files\n  gtfs_list <-\n    purrr::map(gtfs_zip_list, ~ tidytransit::read_gtfs(.)) |> purrr::set_names(gtfs_zip_names)\n  \n  # get service period start and end dates from gtfs files\n  gtfs_service_period <- gtfs_list |>\n    purrr::map( ~ data.frame(service_date = seq(\n      min(ymd(.$.$dates_services$date)),\n      max(ymd(.$.$dates_services$date)),\n    by = \"day\"))) |>\n    dplyr::bind_rows(.id = \"service_name\")\n  \n  # get count of services by day and identify overlaps\n  gtfs_service_overlap <- gtfs_service_period |>\n    dplyr::group_by(service_date) |>\n    dplyr::summarize(count = n()) |>\n    dplyr::mutate(\n      #overlap = case_when(count == length(gtfs_list) ~ 1, TRUE ~ 0)\n      # make more flexible - overlap as equal to max services\n      overlap = dplyr::case_when(count == max(count) ~ 1, TRUE ~ 0))\n  \n  # get a service peak around which to graph\n  service_density <-\n    stats::density(as.numeric(gtfs_service_period$service_date))\n  service_density_peak <-\n    lubridate::as_date(as.integer(service_density$x[which.max(service_density$y)]))\n  \n  # get start and end date of overlap period\n  gtfs_service_overlap_start <- gtfs_service_overlap |>\n    dplyr::filter(count == max(count)) |>\n    dplyr::summarize(min(service_date)) |>\n    dplyr::pull()\n  gtfs_service_overlap_end <- gtfs_service_overlap |>\n    dplyr::filter(count == max(count)) |>\n    dplyr::summarize(max(service_date)) |>\n    dplyr::pull()\n  gtfs_service_overlap_start_month <-\n    lubridate::floor_date(gtfs_service_overlap_start, \"month\")\n  gtfs_service_overlap_end_month <-\n    lubridate::ceiling_date(gtfs_service_overlap_end, \"month\") - days(1)\n  \n  # get special days in format CalendR expects - days from start of calendar period\n  special_days_vector <- gtfs_service_overlap |>\n    dplyr::mutate(special_day = (lubridate::interval(start = gtfs_service_overlap_start_month,\n                                  end = service_date) / lubridate::days(1)) + 1) |>\n    dplyr::filter(overlap == 1)\n  \n  # plot overlap gantt chart\n  overlap_plot <- \n    ggplot2::ggplot(\n      gtfs_service_period |> dplyr::filter(\n        dplyr::between(\n          service_date,\n          left = add_with_rollback(service_density_peak, -base::months(6)),\n          right = add_with_rollback(service_density_peak, base::months(5)))),\n      aes(x = service_name, y = service_date, colour = service_name)) +\n    geom_line(linewidth = 10) +\n    geom_hline(\n      yintercept = as.numeric(gtfs_service_overlap_start),\n      linetype = \"dashed\",\n      colour = \"grey50\") +\n    geom_hline(\n      yintercept = as.numeric(gtfs_service_overlap_end),\n      linetype = \"dashed\",\n      colour = \"grey50\") +\n    coord_flip() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n  \n  # make overlap calendar\n  overlap_calendar <-\n    calendR::calendR(\n      from = gtfs_service_overlap_start_month,\n      to = gtfs_service_overlap_end_month,\n      special.days = special_days_vector$special_day,\n      special.col = \"darkorange2\",\n      subtitle = \"GTFS Service Calendar Overlap\",\n      weeknames = c(\"Mo\", \"Tu\", \"We\", \"Th\", \"Fr\", \"Sa\", \"Su\"))\n  \n  return(list(\"overlap_plot\" = overlap_plot, \"overlap_calendar\" = overlap_calendar))\n}\n```\n:::\n\n\n\nUsing this, we can now check overlap in calendars:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngtfs_overlap <- check_gtfs_overlap(gtfs_folder = r5_graph_path) \n```\n:::\n\n\n\nAnd view the `overlap_plot`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngtfs_overlap |> pluck(\"overlap_plot\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot overlap-1.png){width=672}\n:::\n:::\n\n\n\nLooks good, all of our services overlap around September. We can see the overlapping service calendar further below to pick a departure datetime.\n\n## Elevation\n\nOne last optional step for routing is to collect an elevation surface raster for the study region. {r5r} and R5 can use this to estimate slope-aware travel times for walking. Toronto is pretty flat, particularly compared to say Hong Kong (see Higgins -@higgins2021). I am of the opinion that getting good estimates of slope for walking travel requires a very detailed elevation surface. However, getting a highly detailed elevation surface for the Golden Horseshoe study area would be quite intensive. Because of that, I am going to skip this part. But if you want to get an elevation surface, you can use the {elevatr} package and pass it either the street network {sf} object (slower) or polygons that bound the study area, like the Toronto and Oshawa CMA boundaries you could quickly get from {cancensus}.\n\n# Build Network\n\nWith the OSM data and GTFS files with overlapping transit service calendars collected, we are ready to build the network using {r5r}:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr5_graph <- r5r::setup_r5(data_path = r5_graph_path)\n```\n:::\n\n\n\n# Get Travel Time Matrix\n\nThe first thing we need to calculate transit accessibility is a travel time matrix. For this, {r5r} expects your input origins and destinations to be `POINT` geometries in the WGS 1984 coordinate reference system ([EPSG:4326](https://epsg.io/4326)) with a character field called `id`. We can prepare these now - let's use the CTs as origins and the TAZs (with employment counts) as the destinations:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigin_cts <- census_data_ct |> \n  mutate(id = as.character(geo_uid)) |> # <1>\n  select(id, geometry) |> # <2>\n  st_centroid() |> # <3>\n  st_transform(crs = 4326) # <4>\n\ndestination_tazs <- tts_tazs |> \n  mutate(id = as.character(gta06)) |> \n  select(id, jobs, geometry) |> # <5>\n  st_centroid() |> \n  st_transform(crs = 4326)\n```\n:::\n\n\n1. `mutate` a new `id` field converting `geo_uid` to a character-type column\n2. `select` only the `id` and {sf} `geometry` columns\n3. convert the polygons to centroid points\n4. transform the `geometry` coordinates to WGS 1984\n5. also keep the `jobs` column for the destinations as these are our opportunities\n\nWhich day and time should we use for our `departure_datetime` in {r5r} for transit routing? This is our overlapping service calendar:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngtfs_overlap |> pluck(\"overlap_calendar\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot overlap calendar-1.png){width=672}\n:::\n:::\n\n\n\nHow about September 13, 2016 at 8AM? We can now run the travel time matrix:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nttm <- r5r::travel_time_matrix(\n  r5r_core = r5_graph,\n  origins  = origin_cts,\n  destinations = destination_tazs,\n  mode = c(\"transit\", \"walk\"),\n  departure_datetime = lubridate::ymd_hms(\"2016-09-13 08:00:00\"),\n  max_trip_duration = 120,\n  progress = TRUE)\n```\n:::\n\n\n\nAlthough it is fast, you might want to save yourself from calculating a new travel time matrix in the future by saving it to disk. My method of choice is a `.parquet` file (or an Arrow Dataset for large matrices chunked by something like a region ID key) using the {arrow} package:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nttm |> arrow::write_parquet(sink = fs::path(ttm_path, \"ttm.parquet\"))\n```\n:::\n\n\n\n# Calculate Accessibility\n\nWith the travel time matrix complete, we're finally ready to calculate accessibility. The accessibility analysis you run from this point forward is going to be heavily dependent on your research questions and assumptions about travel behaviour, e.g. is this a (to use the terminology in @paez2012) more normative research question about levels of access that individuals and places should have? Perhaps a cumulative cut-off is appropriate to capture something like grocery stores within 15 minutes of travel. \n\nAre you interested in taking a more positivistic approach by modelling actual travel behaviour patterns? Perhaps a gravity-based approach is best for modelling the potential for interaction with an impedance function that accounts for the decreasing propensity to travel with increasing travel time.\n\n## Using the {accessibility} package\n\nWith that said, one straightforward way of doing this is to use the {accessibility} package prepared by Rafa and the team at Ipea (see [here](https://ipeagit.github.io/accessibility/)) which has a bunch of different options for calculating cumulative, gravity, and even competitive access measures. Let's give this a try with a 45-minute cumulative job accessibility analysis. \n\nFirst we need our travel time matrix from disk:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nttm <- arrow::read_parquet(fs::path(ttm_path, \"ttm.parquet\"))\n```\n:::\n\n\n\nNext, we can use the `cumulative_cutoff()` function to calculate access:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncum_access_45 <- accessibility::cumulative_cutoff(\n  travel_matrix = ttm, # <1>\n  land_use_data = destination_tazs, # <2>\n  opportunity = \"jobs\", # <3>\n  travel_cost = \"travel_time_p50\", # <4>\n  cutoff = 45 # <5>\n)\n```\n:::\n\n\n1. the travel time matrix\n2. the destination data with some opportunity column\n3. the name of the opportunities column in the land use data\n4. the name of the travel time column in the travel time matrix\n5. the cut-off value in minutes of travel time\n\nThis handled a lot of the work for us, joining the destination opportunities to the travel time matrix, calculating the impedance-weighted opportunities, and summing the accessibility scores by the origins. We can see the output visually after joining the `cum_access_45` dataframe back to the original `census_data_ct`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_data_ct <- census_data_ct |> \n  left_join(cum_access_45, by = c(\"geo_uid\" = \"id\")) |> \n  rename(access_jobs_45 = jobs)\n```\n:::\n\n\n\nAnd making a map with {tmap}:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntm_shape(census_data_ct) + \n  tm_fill(\n    fill = \"access_jobs_45\", \n    fill.scale = tm_scale_intervals( \n      n = 10, \n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"accessibility to employment \\n(45-min by transit)\", \n      frame = FALSE \n      )\n    )\n```\n\n::: {.cell-output-display}\n![Accessibility to Employment (2016) within 45-minutes by Transit](index_files/figure-html/fig-access_cum_45-1.png){#fig-access_cum_45 width=672}\n:::\n:::\n\n\n\n## Doing it manually\n\nIf you want to customize your analysis (or are interested in seeing how this all works), you can do this the manual way too. First, join the destination opportunities to the travel time matrix:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nttm <- ttm |> \n  left_join(destination_tazs, by = c(\"to_id\" = \"id\"))\n```\n:::\n\n\n\nSecond, for the impedance function, let's adopt a more positivistic approach and utilize a log-logistic function calibrated to commuting trips in Toronto from @kapatsila2023:\n\n$$\nf = \\frac{1}{ 1+ (\\frac{t_{ij}}{\\text{med}(\\tau)}) ^{\\beta}}\n$$\n\nThe function takes two parameter inputs: $\\text{med}(\\tau)$ corresponds to the median travel time for commuting trips and $beta$ is a decay parameter calibrated to trip flows in the paper. For transit commuting in Toronto, $\\text{med}(\\tau) = 49$ and $beta = 4.4856$. We can re-write this as an R function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_logistic_f <- function(t_ij, med_tau, beta) {\n  1 / (1 + (t_ij / med_tau)^beta)\n}\n```\n:::\n\n\n\nFrom @fig-impedance_fs we can see how the log-logistic function results in a much more continuously-declining weight as travel time increases compared to the cumulative cut-off at 45-minutes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata.frame(t_ij = seq(1, 120, by = .1)) |>\n  mutate(\n    weight_cum_45 = case_when(t_ij <= 45 ~ 1, .default = 0),\n    weight_log_logistic = log_logistic_f(t_ij, med_tau = 49, beta = 4.4856)\n  ) |>\n  pivot_longer(\n    cols = starts_with(\"weight_\"),\n    names_to = \"impedance_f\",\n    names_prefix = \"weight_\",\n    values_to = \"weight\"\n  ) |>\n  ggplot(aes(x = t_ij, y = weight, colour = impedance_f)) +\n  geom_line() +\n  scale_x_continuous(name = \"travel time (minutes)\", limits = c(0, 120), breaks = seq(0, 120, by = 15)) +\n  ylab(\"impedance weight\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![Impedance weight by travel time for the cumulative and log-logistic functions](index_files/figure-html/fig-impedance_fs-1.png){#fig-impedance_fs width=672}\n:::\n:::\n\n\n\nThird, use this function to calculate the impedance-weighted opportunities in the travel time matrix:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nttm <- ttm |>\n  mutate(access_jobs_ll = jobs * log_logistic_f(\n    t_ij = travel_time_p50,\n    med_tau = 49,\n    beta = 4.4856\n  ))\n```\n:::\n\n\n\nFourth, summarize the accessibility values by the origins:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccess_log_logistic <- ttm |> \n  group_by(from_id) |> \n  summarize(access_jobs_ll = sum(access_jobs_ll))\n```\n:::\n\n\n\nFinally, we can now join our accessibility scores back to the original CTs:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_data_ct <- census_data_ct |> \n  left_join(access_log_logistic, by = c(\"geo_uid\" = \"from_id\"))\n```\n:::\n\n\n\nAnd map our results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntm_shape(census_data_ct) + \n  tm_fill(\n    fill = \"access_jobs_ll\", \n    fill.scale = tm_scale_intervals( \n      n = 10, \n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"accessibility to employment \\n(log-logistic function)\", \n      frame = FALSE \n      )\n    )\n```\n\n::: {.cell-output-display}\n![Accessibility to Employment (2016) with Continuous Decay](index_files/figure-html/fig-access_ll-1.png){#fig-access_ll width=672}\n:::\n:::\n\n\n\n# Wrap-up\n\nThere you have it! An accessibility analysis in Toronto from front-to-back done in two ways. {r5r} and R5 are multi-modal, so you can repeat the same steps to analyze walking, cycling, and car travel too, for any other type of origin and/or destination type. This is just a starting-point for more advanced treatments of accessibility analysis - on that front, stay tuned. I have a busy sabbatical year planned on that front!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}