[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher D. Higgins",
    "section": "",
    "text": "About\nI am a quantitative geographer whose research is focused on the relationship between form and function in cities. Using the tools and methods of GIS/geographic data science, I capture and model the physical form and infrastructure of the city, its urban networks and the flows of people, goods, and information they facilitate, and the use of the city as an area or volume for engaging in activities. By providing a window into how the dynamic state of the city impacts and is impacted by social, environmental, and economic processes, my work seeks to better inform policy and planning interventions and promote more sustainable urban outcomes. Research areas to date include spatial and spatio-temporal analyses of:\n\nUrban morphology and urban volumetrics\nAccessibility and travel behaviour\n2D/3D/4D econometrics\nHealth and wellbeing\nGeodemographics and demand modelling\nUrban policy and planning\n\n\n\nPosts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlow Transit: Finch West\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 1, 2025\n\n\nChristopher D. Higgins\n\n\n\n\n\n\n\n\n\n\n\n\nAccessibility Analysis in Toronto\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 21, 2025\n\n\nChristopher D. Higgins\n\n\n\n\n\n\n\n\n\n\n\n\nAccelerating R\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nChristopher D. Higgins\n\n\n\n\n\n\n\n\n\n\n\n\nKing Street Transit Priority Corridor Travel Times\n\n\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 27, 2022\n\n\nChristopher D. Higgins\n\n\n\n\n\n\n\n\n\n\n\n\nSSHRC Funding: Transit, Telework, and Housing Markets\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nAug 25, 2022\n\n\nChristopher D. Higgins\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sshrc_project/index.html",
    "href": "posts/sshrc_project/index.html",
    "title": "SSHRC Funding: Transit, Telework, and Housing Markets",
    "section": "",
    "text": "To kick off the migration of this site to a new Quarto website/blog, I am pleased to announce that my project Transit, Telework, and Housing Markets: Investigating Locational Preferences for Transit-oriented Development in Pre- and Post-Pandemic Canadian Cities received funding from the Social Science and Humanities Research Council (SSHRC) in the last Insight Grant competition. This project is being undertaken with the help of co-Investigators Khandker Nurul Habib, Steven Farber, and Eric J. Miller. Here’s a bit of the project summary. More to come over the next two years!"
  },
  {
    "objectID": "posts/sshrc_project/index.html#project-summary",
    "href": "posts/sshrc_project/index.html#project-summary",
    "title": "SSHRC Funding: Transit, Telework, and Housing Markets",
    "section": "Project Summary",
    "text": "Project Summary\nOver the past several decades, policies that promote higher density, mixed use, compact, and transit-oriented development have been implemented as a means of achieving urban vitality and more sustainable growth around the world. Previous research into property markets and individual and household preferences has demonstrated that there is demand amongst certain segments of the population for living in neighbourhoods that offer rapid transit accessibility and higher-density built environments rich in urban amenities.\nHowever, the global COVID-19 pandemic has resulted in a significant shock to the urban systems of many cities, including those in Canada. Early evidence suggests that social distancing measures and increases in remote work have contributed to plummeting transit ridership and a shift in locational preferences for more suburban development. While it remains unclear how transportation systems and housing markets will adjust as we emerge from the pandemic, its disruptive effects on the urban spatial structure of cities requires further investigation.\nFrom a research perspective, previous work examining how individuals and households value transit accessibility and transit-oriented development in housing markets has been conducted under the assumption that individual or household preferences remain (relatively) static and that there will be demand for the benefits of planning interventions that promote more sustainable forms of development. But what happens in the opposite scenario, where transportation services and urban form remain constant and preferences rapidly change? To answer this question, the proposed research seeks to analyze the relationship between property market indicators, household locational preferences and travel patterns, and the demand for urban and suburban built environments."
  },
  {
    "objectID": "posts/accelerating_r/index.html",
    "href": "posts/accelerating_r/index.html",
    "title": "Accelerating R",
    "section": "",
    "text": "One of my favourite types of data to work with is real estate transactions. There is always something interesting to be learned from using hedonic models to tease out or quantify the implicit preferences of homebuyers (and renters) with respect to the old adage of “location, location, location!”.\nSuch data tend to exhibit strong spatial autocorrelation and its associated ills. In response, I often turn to spatial econometric models that take spatial relationships among observations into account. R has a great ecosystem of packages for spatial econometrics, including the {spatialreg} package that I use often.\nHowever, running these models with many observations can be a slow process in R.\nThis is due to the computational costs associated with the manipulation of (potentially very large) spatial weights matrices. When using R’s default math libraries, this can lead to models taking hours or even days to run. Moreover, there isn’t much benefit to just getting better computer hardware: open up task manager on Windows or Activity Monitor on the Mac and you will see that R is only using one CPU core."
  },
  {
    "objectID": "posts/accelerating_r/index.html#accelerating-r",
    "href": "posts/accelerating_r/index.html#accelerating-r",
    "title": "Accelerating R",
    "section": "Accelerating R",
    "text": "Accelerating R\n\n\n\n\n\n\n\n\nFigure 1: Moar fasteR!\n\n\n\n\n\nUnder the hood, R uses two libraries for performing common mathematical operations: Basic Linear Algebra Subprograms (BLAS) and LAPACK (Linear Algebra Package). R ships with “reference” versions of BLAS and LAPACK that will return identical results across any installation of R, but are otherwise un-optimized (see Eddelbuettel, 2016).\nThankfully, there are several solutions out there for replacing R’s reference libraries with more optimized ones that can lead to dramatic reductions in computational time. Since I use both Windows (Intel processors) and macOS (Intel and M-series processors) in my work, below you can find instructions for speeding up R in those two environments. AFAIK Linux users and those with AMD processors can utilize OpenBLAS to acheve the same results, but I don’t have any experience in linking these libraries with R.\n\n\n\n\n\n\nWarning\n\n\n\nThese solutions require some knowledge of the file system and administrator privileges (Windows) and the Terminal (macOS). Like R itself, I offer no warranty or support of any kind for these changes! Indeed, customizing your R install might (and has in the past - see below) broken some R packages. Moreover, and as the R team cautions, “Note that fast BLAS implementations may give different (and often slightly less accurate) results than the reference BLAS included in R”. It is pretty easy to revert these changes if you need to, but certainly stop here if you aren’t comfortable with the risk.\n\n\n\nWindows and Intel\nMy initial search for ways to speed up R led to me adopting of Microsoft’s R Open, which, by default, used Intel’s Math Kernel Library (MKL). The MKL uses versions of the BLAS and LAPACK libraries that are optimized for fast multithreaded performance on Intel’s processors. Sure enough, this led to a dramatic reduction in computational time associated with running spatial econometric models. However, Microsoft discontinued the R Open project with the last release using R 4.0.2 in 2020.\nAs I further scoured the web for solutions, I came across this stackoverflow thread, which had details for how to manually link Intel’s MKL with more recent versions of R. The write up is a bit complex, with some more recent working solutions now found within comments to the original answers in the thread. So here is a more streamlined version.\nFirst, you will need an install of R on your PC. At the time of writing, the current version is 4.3.1, so my file paths below will use that example. Second, download and install Intel’s MKL, which is now part of their larger oneAPI product. I used the offline installer for Windows found here. This downloads oneMKL version 2023.2.0 (the new 2024 version has a very different file structure that I will need to troubleshoot). With these two software packages installed, now we have to do some manual steps (accepting the administrator rights prompts along the way):\n\ncopy all the files inside the C:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\2023.2.0\\redist\\intel64 folder and paste them inside C:\\Program Files\\R\\R-4.3.1\\bin\\x64\ncopy all the files inside the C:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\2023.2.0\\windows\\redist\\intel64_win\\compiler folder and paste them inside C:\\Program Files\\R\\R-4.3.1\\bin\\x64\nin the C:\\Program Files\\R\\R-4.3.1\\bin\\x64 folder, rename the existing Rblas.dll to something like Rblas_ref.dll\nrename the existing Rlapack.dll to something like Rlapack_ref.dll\nmake two copies of mkl_rt.2.dll\nrename one of the copies Rblas.dll\nrename the other copy Rlapack.dll\n\nLarge math operations should now use all of your CPU’s logical cores.\n\nIssues and Reversion\nThe one issue I have encountered in the past with linking the MKL to R is that the {igraph} package (and those depending on it) stopped working. According to a recent answer on the original stackoverflow thread that inspired this, the issue has recently been addressed.\nTo undo these customizations, you would have to either rename (or delete) the two MKL files we renamed to be Rblas.dll and Rlapack.dll and substitute the original reference files back in by renaming them to their original names.\n\n\n\nmacOS\nAccelerating R on a (newer) Mac is more straightforward, and instructions for doing so are actually contained within the official R for macOS FAQ. R for macOS ships with vecLib, which is part of Apple’s Accelerate framework of optimizations for Apple hardware. All you have to do is tell R to use the vecLib BLAS rather than the reference one.\nTo do so, open Terminal and run:\n\ncd /Library/Frameworks/R.framework/Resources/lib\n\nfollowed by:\n\nln -sf libRblas.vecLib.dylib libRblas.dylib\n\nOn an Intel Mac, this will result in optimized math functions using all available CPU cores. On M-series processors, it looks like this behaves a bit differently, as optimized math routines are instead handled by the specialized AMX co-processor, so you will not see full CPU utilization.\n\nIssues and Reversion\nI have not run into any compatibility issues using R with Apple’s Accelerate framework. To revert the changes, run the following two commands in Terminal to point R back to the original reference BLAS:\n\ncd /Library/Frameworks/R.framework/Resources/lib\nln -sf libRblas.0.dylib libRblas.dylib\n\n\n\n\nChecking that it works\nA simple check to ensure R is using the optimized libraries is to run the following code from this answer on stackoverflow.\n\nm &lt;- 10000\nn &lt;- 2000\nA &lt;- matrix (runif (m*n),m,n)\nsystem.time (S &lt;- svd (A,nu=0,nv=0))\n\n   user  system elapsed \n  2.314   0.167   1.875 \n\n\nOn my PC with an 18-core Intel Xeon processor, this takes about 40 seconds without using the MKL. After linking the libraries, the computation time drops to just over 3 seconds! My MacBook Pro with an M2 processor is even faster at about 2 seconds. In effect, this drops my modelling time for some large spatial econometric models from hours to minutes."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "JPG1400 Advanced Quantitative Methods\nGGRC30 Advanced GIS\nGGRC32 Essential Spatial Analysis"
  },
  {
    "objectID": "teaching.html#current",
    "href": "teaching.html#current",
    "title": "Teaching",
    "section": "",
    "text": "JPG1400 Advanced Quantitative Methods\nGGRC30 Advanced GIS\nGGRC32 Essential Spatial Analysis"
  },
  {
    "objectID": "teaching.html#past",
    "href": "teaching.html#past",
    "title": "Teaching",
    "section": "Past",
    "text": "Past\n\nUniversity of Toronto / UTSC\n\nCITC18 - Urban Transportation Policy Analysis\nGGRD25 - Research Seminar in Urban Spaces\n\n\n\nHong Kong Polytechnic University\n\nBRE217 Planning and Development\nLSGI2B01 Map Reading and Interpretation\nLSGI545 Urban Informatics\nLSGI5462 Urban Applications of GIS and Remote Sensing\n\n\n\nMcMaster University\n\nGEOG3HP3 Population Growth and Aging"
  },
  {
    "objectID": "posts/accessibility_analysis/index.html",
    "href": "posts/accessibility_analysis/index.html",
    "title": "Accessibility Analysis in Toronto",
    "section": "",
    "text": "Over this past academic year, we were honoured to host Dr. Rafael Pereira as a Bousfield Distinguished Visiting Professor in the Department of Geography and Planning. Rafa earned his PhD at Oxford and has published dozens of great academic papers. He is also the lead of the data science team at the Institute for Applied Economic Research (Ipea) which has published the {r5r} package that enables rapid realistic routing on multimodal transport networks by connecting R with the open source R5 Routing Engine from Conveyal. Beyond his Bousfield Lecture on Advancing Urban Accessibility for Inclusive Cities, he also led a crash-course workshop on Urban Accessibility with R where students and practitioners in attendance got some hands-on experience using R for accessibility analysis with some data for Brazil.\nBetween the workshop, my teaching of courses like GGRC30 Advanced GIS and JPG1400 Advanced Quantitative Methods where students are increasingly conducting accessibility analyses and often needing some guidance, and the joking by Steven Farber that I should be local tech support for {r5r}, I’ve put together a collection of the first steps I undertake in nearly every analysis of accessibility in the City of Toronto and larger region. Together, these serve as a nice little introductory vignette to the topic in the local context."
  },
  {
    "objectID": "posts/accessibility_analysis/index.html#origin-places",
    "href": "posts/accessibility_analysis/index.html#origin-places",
    "title": "Accessibility Analysis in Toronto",
    "section": "Origin Places",
    "text": "Origin Places\nPlace-based measures of access require information on the locations of the origin places \\(i\\). One popular option for origin and destination places is Census zones, such as Dissemination Areas (DAs) or Census Tracts (CTs). These can be obtained using the great {cancensus} package. Please see the vignettes from the package documentation to learn more about getting a free API key, finding census datasets, regions, and vectors. First, we need to provide an API key:\n\nset_cancensus_api_key(\"&lt;your API key&gt;\", install = TRUE)\n\nI will also set the cache directory for this project to avoid multiple downloads:\n\nset_cancensus_cache_path(cache_path)\n\n./cache\n\n\nWith this set, we can now get some data for CTs for the Toronto and Oshawa CMAs that traditionally make up the Greater Toronto Area:\n\ncensus_data_ct &lt;- get_census(\n1    dataset = 'CA16',\n2    regions = list(CMA = c(\"35532\", \"35535\")),\n3    level = 'CT',\n4    geo_format = \"sf\",\n    use_cache = TRUE) |&gt; \n5  janitor::clean_names() |&gt;\n6  st_transform(crs = 26917) |&gt;\n7  mutate(population_density = population / shape_area)\n\n\n1\n\nUse the 2016 Census\n\n2\n\nSpecify CMAs as the province code for Ontario (“35”) and the CMA codes for Oshawa (“532”) and Toronto (“535”)\n\n3\n\nGet Census Tracts\n\n4\n\nTell {cancensus} we want the CT geometries in {sf} format\n\n5\n\nOften Census data comes with capital letters and spaces in the names, so use {janitor} to clean the names up\n\n6\n\nTransform the {sf} geometries to the NAD 1983 Zone 17n projection for the region (EPSG code 26917)\n\n7\n\nMutate a population density (in people per \\(km^2\\)) column\n\n\n\n\nLet’s see what this data looks like by mapping our population_density variable using {tmap}:\n\n\nCode\ntm_shape(census_data_ct) + \n  tm_fill(\n1    fill = \"population_density\",\n2    fill.scale = tm_scale_intervals(\n3      n = 10,\n4      style = \"jenks\",\n5      values = \"viridis\"\n      ), \n    fill.legend = tm_legend(\n6      title = \"population density \\n(people per km\\U00B2)\",\n7      frame = FALSE\n      )\n    )\n\n\n\n1\n\ntell {tmap} we want to map the population_density variable\n\n2\n\nuse an interval-based classification scheme\n\n3\n\nwith 10 breaks in the distribution\n\n4\n\nuse the Jenks algorithm to set values for the 10 breaks\n\n5\n\nuse the viridis colour scheme\n\n6\n\nset the legend title, including a line break using \\n and the unicode character \\U00B2 for a superscript of 2\n\n7\n\nturn off the legend frame\n\n\n\n\n\n\n\n\n\n\nFigure 1: Population Densities (2016) for CTs in the Greater Toronto Area\n\n\n\n\n\nWithin the Toronto region, another option for origin and destination places is the traffic analysis zones (TAZs) associated with the Transportation Tomorrow Survey (TTS). This survey covers the larger Greater Golden Horseshoe area with zones roughly similar in size to CTs. A recent package called {TTS2016R} has gathered these zones for easy use in R, and I will filter them down to just the zones in the Toronto and Oshawa CMAs:\n\ntts_tazs &lt;- TTS2016R::ggh_taz |&gt; \n  janitor::clean_names() |&gt;\n  sf::st_transform(crs = 26917) |&gt; \n  filter(cmauid %in% c(\"532\", \"535\")) |&gt; \n  mutate(\n    workers_density = workers / area,\n    jobs_density = jobs / area)\n\nThe {TTS2016R} package focuses on worker and job counts. Here’s a map of the density of the working population:\n\n\nCode\ntm_shape(tts_tazs) + \n  tm_fill(\n    fill = \"workers_density\", \n    fill.scale = tm_scale_intervals(\n      n = 10, #\n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"worker density \\n(people per km\\U00B2)\", \n      frame = FALSE \n      )\n    )\n\n\n\n\n\n\n\n\nFigure 2: Worker Densities (2016) for TAZs in the Greater Toronto Area"
  },
  {
    "objectID": "posts/accessibility_analysis/index.html#destination-opportunities",
    "href": "posts/accessibility_analysis/index.html#destination-opportunities",
    "title": "Accessibility Analysis in Toronto",
    "section": "Destination Opportunities",
    "text": "Destination Opportunities\nWe also need some destination places \\(j\\), as well as some representation of the opportunities \\(O\\) at the destinations. There are many different types of destination opportunities that can be considered for accessibility analysis. A popular option is employment counts at the destination zones. This data can be a bit hard to track down, but it is available for DAs from the 2016 Census via a custom extract of the Employed Labour Force by Place of Work hosted on Borealis here. This data comes as an Excel spreadsheet and will require some prep outside of R.\nThe TTS also captures employment counts at the destination TAZs and the survey data is available now for 2022 via the Data Management Group. Helpfully, the {TTS2016R} package offers job counts from the 2016 TTS at the TAZ level - here’s a map of employment counts for the TAZs that make up the Toronto and Oshawa CMAs:\n\n\nCode\ntm_shape(tts_tazs) + \n  tm_fill(\n    fill = \"jobs\", \n    fill.scale = tm_scale_intervals(\n      n = 10, #\n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"employment count\", \n      frame = FALSE \n      )\n    )\n\n\n\n\n\n\n\n\nFigure 3: Employment Counts (2016) for TAZs in the Greater Toronto Area\n\n\n\n\n\nBeyond employment counts, other popular destinations include point of interest (POI) data for things like hospitals, grocery stores, etc. Often in these cases, the “opportunity” weight for a POI will be equal to one so that the accessibility analysis counts the number of a given POI type accessible from an origin place. One source for this for University students is DMTI’s Enhanced Points of Interest database, which can be found on the Scholar’s GeoPortal after logging-in with your university credentials. This database can be filtered by the North American Industry Classification System (NAICS) codes for the POIs. We used the DMTI data in Yu and Higgins (2024).\nAlternatively, POI data collected from sources like Microsoft and Meta is also now available as an open data product through Overture Maps and can accessed via new R packages such as the {overtureR} package. Other POIs like parks and schools can be found from municipal (e.g. Toronto), provincial (Ontario), and federal open data portals. For example, Statistics Canada has been collecting a range of open data products through their Linkable Open Data Environment project."
  },
  {
    "objectID": "posts/accessibility_analysis/index.html#openstreetmap-file",
    "href": "posts/accessibility_analysis/index.html#openstreetmap-file",
    "title": "Accessibility Analysis in Toronto",
    "section": "OpenStreetMap File",
    "text": "OpenStreetMap File\nWith the origin places and destination opportunities collected, next you need the core components that R5 and {r5r} need to create a routable network. The first of these is a street network from OpenStreetMap (OSM). The way I do this is through the {osmextract} package, which allows you to search by a place name. Pre-defined OSM extracts can be found for major places around the world. These are great because they are relatively small and won’t cause an error associated with the maximum study area size of 975,000 \\(km^2\\) in R5. Outside of R, you can also find metro area extracts from Interline (API key required).\nHowever, if a place extract does not exist, you might have to move up to the next level of geography, which, in the Canadian case, could entail downloading gigabytes of OSM data for an entire province. This is a much trickier situation and requires the use of tools like {rosmosis} (the easier but slower tool, see here) or {rosmium} (the faster but harder to install and use tool, see here) to clip the OSM network to a bounding box in R.\nIn the Toronto case, there are two good extract options. The first is a ~70mb extract available from “bbbike”, but it really only covers the City of Toronto.\n\nosmextract::oe_match(\"Toronto\")\n\nNo exact match found for place = Toronto and provider = geofabrik. Best match is Morocco. \nChecking the other providers.\n\n\nAn exact string match was found using provider = bbbike.\n\n\n$url\n[1] \"https://download.bbbike.org/osm/bbbike/Toronto/Toronto.osm.pbf\"\n\n$file_size\n[1] 72915302\n\n\nFor working in the GTA and/or to include cities like Hamilton, etc., there is also a great extract for the “Golden Horseshoe” available from “openstreetmap_fr”:\n\nosmextract::oe_match(\"Golden Horseshoe\")\n\nNo exact match found for place = Golden Horseshoe and provider = geofabrik. Best match is Centro-Oeste. \nChecking the other providers.\n\n\nAn exact string match was found using provider = openstreetmap_fr.\n\n\n$url\n[1] \"http://download.openstreetmap.fr/extracts/north-america/canada/ontario/golden_horseshoe-latest.osm.pbf\"\n\n$file_size\n[1] 156983172\n\n\nLet’s download the OSM extract for the Golden Horseshoe to our r5_graph_path by providing the “openstreetmap_fr” URL to the oe_download() function:\n\nosmextract::oe_download(\n  file_url = \"http://download.openstreetmap.fr/extracts/north-america/canada/ontario/golden_horseshoe-latest.osm.pbf\",\n  provider = \"openstreetmap_fr\",\n  download_directory = r5_graph_path\n)\n\nWe can read in the lines layer of this OSM .pbf file as a simple features {sf} object using oe_read(), which translates the .pbf into a geopackage (.gpkg) in our r5_graph_path folder:\n\nosm_lines_sf &lt;- osmextract::oe_read(\n  file_path = fs::path(r5_graph_path, \"openstreetmap_fr_golden_horseshoe-latest.osm.pbf\"),\n  layer = \"lines\")\n\nThe corresponding gpkg file was already detected. Skip vectortranslate operations.\n\n\nReading layer `lines' from data source \n  `/Users/chris/Library/CloudStorage/OneDrive-UniversityofToronto/GitHub/higgicd.github.io/posts/accessibility_analysis/r5_graph/openstreetmap_fr_golden_horseshoe-latest.gpkg' \n  using driver `GPKG'\nSimple feature collection with 939997 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -82.36947 ymin: 42.51469 xmax: -76.16424 ymax: 44.98971\nGeodetic CRS:  WGS 84\n\n\nWhen plotting the major roadways on the map, we can see that this extract covers this part of Southern Ontario:\n\n\nCode\ntm_shape(osm_lines_sf |&gt; \n           filter(highway %in% c(\"motorway\", \"primary\", \"secondary\"))) +\n  tm_lines(\n    col = \"highway\", \n    col.scale = tm_scale_categorical(values = \"plasma\"),\n    col.legend = tm_legend(\n      title = \"street type\", \n      frame = FALSE \n      )\n    )\n\n\n\n\n\n\n\n\nFigure 4: Major Street Types from the Golden Horseshoe OSM Extract"
  },
  {
    "objectID": "posts/accessibility_analysis/index.html#gtfs-data",
    "href": "posts/accessibility_analysis/index.html#gtfs-data",
    "title": "Accessibility Analysis in Toronto",
    "section": "GTFS Data",
    "text": "GTFS Data\nThe second key input is General Transit Feed Specification (GTFS) static schedule files to enable transit routing. These can be found from a variety of sources, including municipal or agency open data portals. However, these are often only the most current schedule files. To better align with our 2016 place data, we can download GTFS files from other archival sources including:\n\ntransitfeeds which is great for historical data but is no longer current\nmobility database which is replacing transitfeeds (I have not used yet)\ntransit.land a newer source, but it does require an API key to download historical files (free for hobbyist and academic use, see bottom of the pricing page)\ngtfs exchange was one of the earliest archives, you could still try to access it through the Internet Archive\n\nFor Toronto, the fragmentation of the region means there are a number of different transit providers (if you think Toronto is annoying, see Montreal!). This makes things tricky in that first you have to track them all down (the browse by region in transitfeeds and transit.land is good for this) but also - and this is critical - they have to all have some service calendar days that align with each other.\nIn the Toronto case, you can reliably get GTFS feeds for the major providers across the region back to about fall 2016 from transitfeeds, and I have collected the download URLs for feeds around September 2016 into this named list:\n\ngtfs_list &lt;- list(\n  \"brampton\" = \"https://transitfeeds.com/p/brampton-transit/35/20160818/download\",\n  \"burlington\" = \"https://transitfeeds.com/p/burlington-transit/294/20160906/download\",\n  \"durham\" = \"https://transitfeeds.com/p/durham-region-transit/642/20160824/download\",\n  \"go\" = \"https://transitfeeds.com/p/go-transit/32/20160906/download\",\n  \"mississauga\" = \"https://transitfeeds.com/p/miway/641/20160907/download\",\n  \"oakville\" = \"https://transitfeeds.com/p/oakville-transit/615/20160901/download\",\n  \"toronto\" = \"https://transitfeeds.com/p/ttc/33/20160829/download\",\n  \"york\" = \"https://transitfeeds.com/p/york-regional-transit/34/20160904/download\"\n)\n\nNext, I will pass this list to an iwalk() function from {purrr} for iteration with an index. The function iterates the req_perform() function for downloading files from the {httr} package and saves the output to the r5_graph_path folder:\n\n1gtfs_list |&gt;\n2  purrr::iwalk(~ httr2::req_perform(\n3    req = httr2::request(.x) |&gt;\n4      httr2::req_cache(path = cache_path),\n5    path = fs::path(r5_graph_path, paste0(.y, \".zip\")))\n  )\n\n\n1\n\npass the gtfs_list into the pipeline\n\n2\n\ncall the req_perform() function inside iwalk()\n\n3\n\ncreate a request object using the elements of the list (the URLs) represented as .x\n\n4\n\nI have added a cache option to cache the files to the cache_path\n\n5\n\nname the output .zip files using the list index (the names) represented as .y and save to the r5_graph_path\n\n\n\n\nWe should now have our GTFS .zip files in our r5_graph_path folder:\n\nfs::dir_tree(r5_graph_path, glob = \"*.zip\")\n\n./r5_graph\n├── brampton.zip\n├── burlington.zip\n├── durham.zip\n├── go.zip\n├── mississauga.zip\n├── oakville.zip\n├── toronto.zip\n└── york.zip\n\n\nAs a last step, let’s verify that our service calendars do overlap. If they don’t, the departure datetime that we pick for transit routing will omit any services that don’t have scheduled operations on that day. To facilitate this, I have created a function called check_gtfs_overlap() that reads in a folder of GTFS files and returns their service calendars:\n\n\nCode\ncheck_gtfs_overlap &lt;- function(gtfs_folder) {\n  # get a list of gtfs zip files in the gtfs directory\n  gtfs_zip_list &lt;- fs::dir_ls(gtfs_folder, regexp = \"*.zip\")\n  \n  # get provider names from the file list\n  gtfs_zip_names &lt;- fs::path_file(gtfs_zip_list) |&gt; fs::path_ext_remove()\n  \n  # read in gtfs files\n  gtfs_list &lt;- purrr::map(gtfs_zip_list, ~ tidytransit::read_gtfs(.)) |&gt; purrr::set_names(gtfs_zip_names)\n  \n  # get service period start and end dates from gtfs files\n  gtfs_service_period &lt;- gtfs_list |&gt;\n    purrr::map( ~ data.frame(service_date = seq(min(\n      ymd(.$.$dates_services$date)\n    ), max(\n      ymd(.$.$dates_services$date)\n    ), by = \"day\"))) |&gt;\n    dplyr::bind_rows(.id = \"service_name\")\n  \n  # get count of services by day and identify overlaps\n  gtfs_service_overlap &lt;- gtfs_service_period |&gt;\n    dplyr::group_by(service_date) |&gt;\n    dplyr::summarize(count = n()) |&gt;\n    dplyr::mutate(\n      #overlap = case_when(count == length(gtfs_list) ~ 1, TRUE ~ 0)\n      # make more flexible - overlap as equal to max services\n      overlap = dplyr::case_when(count == max(count) ~ 1, TRUE ~ 0)\n    )\n  \n  # get a service peak around which to graph\n  service_density &lt;- stats::density(as.numeric(gtfs_service_period$service_date))\n  \n  service_density_peak &lt;- lubridate::as_date(as.integer(service_density$x[which.max(service_density$y)]))\n  \n  # get start and end date of overlap period\n  gtfs_service_overlap_start &lt;- gtfs_service_overlap |&gt;\n    dplyr::filter(count == max(count)) |&gt;\n    dplyr::summarize(min(service_date)) |&gt;\n    dplyr::pull()\n  \n  gtfs_service_overlap_end &lt;- gtfs_service_overlap |&gt;\n    dplyr::filter(count == max(count)) |&gt;\n    dplyr::summarize(max(service_date)) |&gt;\n    dplyr::pull()\n  \n  gtfs_service_overlap_start_month &lt;- lubridate::floor_date(gtfs_service_overlap_start, \"month\")\n  \n  gtfs_service_overlap_end_month &lt;- lubridate::ceiling_date(gtfs_service_overlap_end, \"month\") - days(1)\n  \n  # 2. plot overlap gantt chart\n  overlap_plot &lt;- gtfs_service_period |&gt;\n    dplyr::filter(\n      dplyr::between(\n        service_date,\n        left = add_with_rollback(service_density_peak, -base::months(6)),\n        right = add_with_rollback(service_density_peak, base::months(5))\n      )\n    ) |&gt;\n    ggplot2::ggplot(aes(x = service_name, y = service_date, colour = service_name)) +\n    geom_line(linewidth = 10) +\n    geom_hline(yintercept = gtfs_service_overlap_start,\n               linetype = \"dashed\",\n               colour = \"grey50\") +\n    geom_hline(yintercept = gtfs_service_overlap_end,\n               linetype = \"dashed\",\n               colour = \"grey50\") +\n    coord_flip() +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n  \n  # 3. plot overlap calendar\n  overlap_days &lt;- gtfs_service_overlap |&gt;\n    dplyr::filter(overlap == 1)\n  \n  overlap_calendar &lt;- tibble(\n    service_date = seq(\n      gtfs_service_overlap_start_month,\n      gtfs_service_overlap_end_month,\n      by = \"day\"\n    )\n  ) |&gt;\n    mutate(\n      day = mday(service_date),\n      month = month(service_date, label = TRUE, abbr = FALSE),\n      weekday = wday(service_date, label = TRUE, abbr = TRUE),\n      \n      # Calculate week of month (example logic, adjust as needed)\n      week_of_month = ceiling((day + wday(floor_date(service_date, \"month\"), week_start = 7) - 1) / 7)\n    ) |&gt;\n    # Ensure correct weekday order (e.g., Mon-Sun)\n    mutate(weekday = factor(\n      weekday,\n      levels = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\n    )) |&gt;\n    left_join(overlap_days, by = \"service_date\") |&gt;\n    mutate(overlap = replace_na(overlap, 0)) |&gt;\n    ggplot(aes(\n      x = weekday,\n      y = -week_of_month, # -week_of_month to arrange weeks top-down\n      fill = factor(overlap)\n    )) + \n    geom_tile(color = \"white\") + # Color for tile borders\n    geom_text(aes(label = day), size = 3, color = \"black\") +\n    facet_wrap( ~ month, ncol = 3, scales = \"free\") + # Facet by month\n    scale_fill_manual(\n      values = c(\"0\" = \"grey95\", \"1\" = \"darkorange2\"),\n      labels = c(\"0\" = \"Limited service\", \"1\" = \"Overlapping service days\"),\n      name = \"Service level\",\n    ) +\n    labs(title = \"GTFS Service Calendar Overlap\") +\n    theme_minimal() +\n    theme(\n      axis.title = element_blank(),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(vjust = 1),\n      axis.ticks = element_blank(),\n      panel.grid = element_blank(),\n      strip.text = element_text(face = \"bold\", size = 12),\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      panel.border = element_rect(\n        color = \"grey90\",\n        fill = NA,\n        linewidth = 0.8\n      ),\n      legend.position = \"bottom\"\n    )\n  \n  return(list(\"overlap_plot\" = overlap_plot, \"overlap_calendar\" = overlap_calendar))\n}\n\n\nUsing this, we can now check overlap in calendars:\n\ngtfs_overlap &lt;- check_gtfs_overlap(gtfs_folder = r5_graph_path) \n\nAnd view the overlap_plot:\n\ngtfs_overlap |&gt; pluck(\"overlap_plot\")\n\n\n\n\n\n\n\n\nLooks good, all of our services overlap around September. We can see the overlapping service calendar further below to pick a departure datetime."
  },
  {
    "objectID": "posts/accessibility_analysis/index.html#elevation",
    "href": "posts/accessibility_analysis/index.html#elevation",
    "title": "Accessibility Analysis in Toronto",
    "section": "Elevation",
    "text": "Elevation\nOne last optional step for routing is to collect an elevation surface raster for the study region. {r5r} and R5 can use this to estimate slope-aware travel times for walking. Toronto is pretty flat, particularly compared to say Hong Kong (see Higgins (2021)). I am of the opinion that getting good estimates of slope for walking travel requires a very detailed elevation surface. However, getting a highly detailed elevation surface for the Golden Horseshoe study area would be quite intensive. Because of that, I am going to skip this part. But if you want to get an elevation surface, you can use the {elevatr} package and pass it either the street network {sf} object (slower) or polygons that bound the study area, like the Toronto and Oshawa CMA boundaries you could quickly get from {cancensus}."
  },
  {
    "objectID": "posts/accessibility_analysis/index.html#using-the-accessibility-package",
    "href": "posts/accessibility_analysis/index.html#using-the-accessibility-package",
    "title": "Accessibility Analysis in Toronto",
    "section": "Using the {accessibility} package",
    "text": "Using the {accessibility} package\nWith that said, one straightforward way of doing this is to use the {accessibility} package prepared by Rafa and the team at Ipea (see here) which has a bunch of different options for calculating cumulative, gravity, and even competitive access measures. Let’s give this a try with a 45-minute cumulative job accessibility analysis.\nFirst we need our travel time matrix from disk:\n\nttm &lt;- arrow::read_parquet(fs::path(ttm_path, \"ttm.parquet\"))\n\nNext, we can use the cumulative_cutoff() function to calculate access:\n\ncum_access_45 &lt;- accessibility::cumulative_cutoff(\n1  travel_matrix = ttm,\n2  land_use_data = destination_tazs,\n3  opportunity = \"jobs\",\n4  travel_cost = \"travel_time_p50\",\n5  cutoff = 45\n)\n\n\n1\n\nthe travel time matrix\n\n2\n\nthe destination data with some opportunity column\n\n3\n\nthe name of the opportunities column in the land use data\n\n4\n\nthe name of the travel time column in the travel time matrix\n\n5\n\nthe cut-off value in minutes of travel time\n\n\n\n\nThis handled a lot of the work for us, joining the destination opportunities to the travel time matrix, calculating the impedance-weighted opportunities, and summing the accessibility scores by the origins. We can see the output visually after joining the cum_access_45 dataframe back to the original census_data_ct:\n\ncensus_data_ct &lt;- census_data_ct |&gt; \n  left_join(cum_access_45, by = c(\"geo_uid\" = \"id\")) |&gt; \n  rename(access_jobs_45 = jobs)\n\nAnd making a map with {tmap}:\n\n\nCode\ntm_shape(census_data_ct) + \n  tm_fill(\n    fill = \"access_jobs_45\", \n    fill.scale = tm_scale_intervals( \n      n = 10, \n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"accessibility to employment \\n(45-min by transit)\", \n      frame = FALSE \n      )\n    )\n\n\n\n\n\n\n\n\nFigure 5: Accessibility to Employment (2016) within 45-minutes by Transit"
  },
  {
    "objectID": "posts/accessibility_analysis/index.html#doing-it-manually",
    "href": "posts/accessibility_analysis/index.html#doing-it-manually",
    "title": "Accessibility Analysis in Toronto",
    "section": "Doing it manually",
    "text": "Doing it manually\nIf you want to customize your analysis (or are interested in seeing how this all works), you can do this the manual way too. First, join the destination opportunities to the travel time matrix:\n\nttm &lt;- ttm |&gt; \n  left_join(destination_tazs, by = c(\"to_id\" = \"id\"))\n\nSecond, for the impedance function, let’s adopt a more positivistic approach and utilize a log-logistic function calibrated to commuting trips in Toronto from Kapatsila et al. (2023):\n\\[\nf = \\frac{1}{ 1+ (\\frac{t_{ij}}{\\text{med}(\\tau)}) ^{\\beta}}\n\\]\nThe function takes two parameter inputs: \\(\\text{med}(\\tau)\\) corresponds to the median travel time for commuting trips and \\(\\beta\\) is a decay parameter calibrated to trip flows in the paper. For transit commuting in Toronto, \\(\\text{med}(\\tau) = 49\\) and \\(\\beta = 4.4856\\). We can re-write this as an R function:\n\nlog_logistic_f &lt;- function(t_ij, med_tau, beta) {\n  1 / (1 + (t_ij / med_tau)^beta)\n}\n\nFrom Figure 6 we can see how the log-logistic function results in a much more continuously-declining weight as travel time increases compared to the cumulative cut-off at 45-minutes.\n\n\nCode\ndata.frame(t_ij = seq(1, 120, by = .1)) |&gt;\n  mutate(\n    weight_cum_45 = case_when(t_ij &lt;= 45 ~ 1, .default = 0),\n    weight_log_logistic = log_logistic_f(t_ij, med_tau = 49, beta = 4.4856)\n  ) |&gt;\n  pivot_longer(\n    cols = starts_with(\"weight_\"),\n    names_to = \"impedance_f\",\n    names_prefix = \"weight_\",\n    values_to = \"weight\"\n  ) |&gt;\n  ggplot(aes(x = t_ij, y = weight, colour = impedance_f)) +\n  geom_line() +\n  scale_x_continuous(name = \"travel time (minutes)\", limits = c(0, 120), breaks = seq(0, 120, by = 15)) +\n  ylab(\"impedance weight\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nFigure 6: Impedance weight by travel time for the cumulative and log-logistic functions\n\n\n\n\n\nThird, use this function to calculate the impedance-weighted opportunities in the travel time matrix:\n\nttm &lt;- ttm |&gt;\n  mutate(access_jobs_ll = jobs * log_logistic_f(\n    t_ij = travel_time_p50,\n    med_tau = 49,\n    beta = 4.4856\n  ))\n\nFourth, summarize the accessibility values by the origins:\n\naccess_log_logistic &lt;- ttm |&gt; \n  group_by(from_id) |&gt; \n  summarize(access_jobs_ll = sum(access_jobs_ll))\n\nFinally, we can now join our accessibility scores back to the original CTs:\n\ncensus_data_ct &lt;- census_data_ct |&gt; \n  left_join(access_log_logistic, by = c(\"geo_uid\" = \"from_id\"))\n\nAnd map our results:\n\n\nCode\ntm_shape(census_data_ct) + \n  tm_fill(\n    fill = \"access_jobs_ll\", \n    fill.scale = tm_scale_intervals( \n      n = 10, \n      style = \"jenks\", \n      values = \"viridis\" \n      ), \n    fill.legend = tm_legend(\n      title = \"accessibility to employment \\n(log-logistic function)\", \n      frame = FALSE \n      )\n    )\n\n\n\n\n\n\n\n\nFigure 7: Accessibility to Employment (2016) with Continuous Decay"
  },
  {
    "objectID": "posts/slow_transit/index.html",
    "href": "posts/slow_transit/index.html",
    "title": "Slow Transit: Finch West",
    "section": "",
    "text": "The excitement is palpable – new rapid transit is opening in Toronto. After 7 years of construction, the $3.6 billion Finch West LRT will open its doors on December 7, beating out another line just to the south who shall not be named.\nFigure 1: ChatGPT’s “It’s Happening meme with the Finch West LRT”\nThat excitement for new rapid transit died a little bit when early looks at the new General Transit Feed Specification (GTFS) schedules posted by the TTC this past weekend revealed that the scheduled travel times along the line correspond to an average speed of 13.53kph or about a 45 minute trip from end to end. Per Steve Munro’s reporting, this speed is actually slower than the Finch bus that the LRT is supposed to replace, and certainly slower than the ~18kph and 34 minute travel time from milestone testing.\nNow, who knows why these posted GTFS travel times differ so widely from the milestone travel times where vehicles were reaching up to 60kph in some segments. GTFS schedules are a bit opaque in that they reflect static travel times that may or may not be optimistic/pessimistic. Previous work in Toronto has already shown that these schedules can differ quite dramatically from observed vehicle movements captured by GTFS Realtime data (Wessel and Farber 2019).\nPerhaps they are overly conservative for a soft opening. Perhaps they are just true based on how the TTC feels comfortable operating the line. Perhaps they are purposefully slow to help make a point about signal priority and we are taking the bait. I suppose all we can do is see how things settle in when proper revenue service begins.\nIn any case, while travel times are one thing, naturally I wanted to see the impact on accessibility – the potential to reach things using the transportation network – relative to the original bus service and considering these posted and potential speeds. See my previous post on Accessibility Analysis in Toronto for some background on that concept and a full walk through on obtaining data and running an analysis.\nAs someone currently writing a book on accessibility analysis with R, the opening of Finch West LRT presents a fun opportunity to explore routing, accessibility, and some simple GTFS editing to speed up a service."
  },
  {
    "objectID": "posts/slow_transit/index.html#setup",
    "href": "posts/slow_transit/index.html#setup",
    "title": "Slow Transit: Finch West",
    "section": "Setup",
    "text": "Setup\nLet’s pick up where the previous post left off and set up our environment. First, allocate some memory to Java for \\(R^5\\) and {r5r} to work and load a few key packages:\n\n# allocate java memory for r5r\noptions(java.parameters = \"-Xmx8G\")\n\n# load base packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\nDownload Data\nSecond, we need a few key data inputs to run an accessibility analysis around the Finch West LRT, including an OpenStreetMap (OSM) road network, GTFS transit schedules, and data on the origins and destinations for the analysis. We can save our major data downloads to a new ./data folder:\n\ndata_path &lt;- fs::dir_create(\"./data\")\n\nNow on to the OSM data:\n\nosmextract::oe_download(\n  file_url = \"http://download.openstreetmap.fr/extracts/north-america/canada/ontario/golden_horseshoe-latest.osm.pbf\",\n  provider = \"openstreetmap_fr\",\n  download_directory = data_path\n)\n\nFor GTFS feeds, I will download two different schedule files. One for November-December 2018 with the Finch bus service before construction started and the newest GTFS file from the TTC with the new Finch LRT, both from transit.land. You would need an API key to download these yourself (free for hobbyist use), which is defined here:\n\ntransit_land_api_key &lt;- \"your_api_key_here\" \n\nNext, set up a named list with the GTFS feed SHA1 IDs:\n\ngtfs_list &lt;- list(\n  \"ttc_2018\" = \"91d9cf49675be7b18d921748a573e4f7289e8df6\",\n  \"ttc_2025\" = \"8574169d5cc56d5563f492102d4197549820ad5d\"\n)\n\nFinally, use an iterative walk() from {purrr} to download the feeds:\n\n\nCode\npurrr::iwalk(gtfs_list,\n             ~ httr2::req_perform(\n               req = httr2::request(\n                 paste0(\n                   \"https://transit.land/api/v2/rest/feed_versions/\",\n                   .x,\n                   \"/download?apikey=\",\n                   transit_land_api_key\n                 )\n               ),\n               path = fs::path(data_path, paste0(.y, \".zip\"))\n             ))\n\n\nFor origin data I will use 2021 Census Dissemination Blocks (DBs). We need to set up our {cancensus} API key and cache folder:\n\ncancensus::set_cancensus_api_key(\"&lt;your API key&gt;\")\ncancensus::set_cancensus_cache_path(data_path)\n\nNow we can get them using this code:\n\ncensus_data_db &lt;- cancensus::get_census(\n    dataset = 'CA21',\n    regions = list(CSD = c(\"3520005\")),\n    level = 'DB',\n    geo_format = \"sf\",\n    use_cache = TRUE) |&gt; \n  janitor::clean_names() |&gt;\n  st_transform(crs = 26917) |&gt;\n  mutate(population_density = population / shape_area)\n\nFor the destinations, let’s keep it simple and use employment counts captured by the 2016 Transportation Tomorrow Survey (TTS) using the {TTS2016R} package (which you can install from the github repository using {remotes}):\n\ntts_tazs &lt;- TTS2016R::ggh_taz |&gt; \n  janitor::clean_names() |&gt;\n  sf::st_transform(crs = 26917) |&gt; \n  mutate(id = gta06)\n\nFinally, to aid with mapping, let’s get the most recent existing and future rapid transit lines shapefile from Metrolinx:\n\nmetrolinx_url &lt;- \"https://assets.metrolinx.com/raw/upload/v1693928524/Documents/Metrolinx/Open%20Data/September%205%2C%202023%20-%20FRTN/GIS_Data_-_Future_Transit_Network_-_2023-05-12.zip\"\n\nutils::download.file(\n  metrolinx_url,\n  destfile = fs::path(data_path, \"metrolinx_rt.zip\")\n  )\n\nzip::unzip(\n  zipfile = fs::path(data_path, \"metrolinx_rt.zip\"),\n  exdir = data_path\n  )\n\nThese data can now be read in as {sf} objects and we can extract the existing rapid transit lines and the Line 6 Finch West LRT:\n\nrt_lines &lt;- st_read(\"./data/RTP_TRANSIT_NETWORK.shp\") |&gt; \n  st_transform(crs = 26917) |&gt; \n  janitor::clean_names()\n  \nrt_stops &lt;- st_read(\"./data/RTP_POINTS.shp\") |&gt; \n  st_transform(crs = 26917) |&gt; \n  janitor::clean_names()\n\nfwlrt_line &lt;- rt_lines |&gt; \n  filter(name == \"Finch West LRT\")\n\nfwlrt_stops &lt;- rt_stops |&gt; \n  filter(name == \"Finch West LRT\")\n\nexisting_rt &lt;- rt_lines |&gt;\n  filter(\n    status == \"Existing\",\n    technology == \"Subway\" | str_starts(technology, \"GO\")\n    )\n\nOK, let’s see what we are working with by making a map of population counts around the Finch West LRT:\n\n\nCode\nfwlrt_line_buffer &lt;- fwlrt_line |&gt; \n  st_buffer(dist = 1000)\n\ncensus_data_db_fwlrt &lt;- census_data_db |&gt; \n  mutate(orig_area = st_area(geometry)) |&gt; \n  st_intersection(fwlrt_line_buffer) |&gt; \n  mutate(\n    id = geo_uid, \n    new_area = st_area(geometry),\n    area_ratio = new_area/orig_area,\n    aw_population = population * area_ratio)\n\ntm_shape(census_data_db, bbox = fwlrt_line_buffer) + tm_polygons(col = \"grey99\", fill = \"grey95\") +\n  tm_shape(census_data_db_fwlrt) +\n  tm_fill(\n    fill = \"aw_population\",\n    fill.scale = tm_scale_intervals(\n      n = 10,\n      style = \"jenks\",\n      values = \"viridis\"\n    ),\n    fill_alpha = .85,\n    fill.legend = tm_legend(\n      title = \"population count\",\n      frame = FALSE\n    )\n  ) +\n  tm_shape(existing_rt) + tm_lines(col = \"grey35\", lwd = 4) +\n  tm_shape(fwlrt_line) + tm_lines(col = \"grey\", lwd = 4) +\n  tm_shape(fwlrt_stops) + tm_dots(size = .5, fill = \"grey\") +\n  tm_shape(fwlrt_stops) + tm_dots(size = .15, fill = \"white\")\n\n\n\n\n\n\n\n\nFigure 2: Population Counts (2021) for DAs in the City of Toronto\n\n\n\n\n\n\n\nBuild Network\nCopy the OSM and GTFS data to a new ttc_prepost folder to store our \\(R^5\\) network:\n\nttc_prepost_network_path &lt;- fs::dir_create(\"./r5_networks/ttc_prepost\")\n\nfs::file_copy(\n  path = fs::path(data_path, \"openstreetmap_fr_golden_horseshoe-latest.osm.pbf\"),\n  new_path = ttc_prepost_network_path,\n  overwrite = TRUE\n)\n\nfs::file_copy(\n  path = fs::path(data_path, \"ttc_2018.zip\"),\n  new_path = ttc_prepost_network_path,\n  overwrite = TRUE\n)\n\nfs::file_copy(\n  path = fs::path(data_path, \"ttc_2025.zip\"),\n  new_path = ttc_prepost_network_path,\n  overwrite = TRUE\n)\n\nThen build the network using {r5r}:\n\nttc_prepost_network &lt;- r5r::build_network(data_path = ttc_prepost_network_path)"
  },
  {
    "objectID": "posts/slow_transit/index.html#pre--and-post-accessibility",
    "href": "posts/slow_transit/index.html#pre--and-post-accessibility",
    "title": "Slow Transit: Finch West",
    "section": "Pre- and Post-Accessibility",
    "text": "Pre- and Post-Accessibility\n\nTravel Times\nHow is Line 6 performing in terms of travel times? With the network built, let’s try some simple routes between Humber College and Finch West subway station at 8:00:00 AM on a typical Tuesday in December to see the difference in travel times from 2018 versus 2025 with the Finch West LRT open:\n\n\nCode\nhumber_point &lt;- fwlrt_stops |&gt; \n  filter(location_n == \"Humber College\") |&gt; \n  mutate(id = 1) |&gt; \n  st_transform(crs = 4326)\n\nfinch_west_point &lt;- fwlrt_stops |&gt; \n  filter(location_n == \"Finch West\") |&gt; \n  mutate(id = 2) |&gt; \n  st_transform(crs = 4326)\n\nroute_pre_eb &lt;- r5r::detailed_itineraries(\n  r5r_network = ttc_prepost_network,\n  origins = humber_point,\n  destinations = finch_west_point,\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2018-12-11 08:00:00\"),\n  shortest_path = FALSE\n) |&gt; filter(option == 1) |&gt; \n  mutate(scenario = \"pre\", route = \"36\", direction = \"eastbound\")\n\n# 55405 is Finch West route 36 in routes.txt\n\nroute_pre_wb &lt;- r5r::detailed_itineraries(\n  r5r_network = ttc_prepost_network,\n  origins = finch_west_point,\n  destinations = humber_point,\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2018-12-11 08:00:00\"),\n  shortest_path = FALSE\n) |&gt; filter(option == 1) |&gt; \n  mutate(scenario = \"pre\", route = \"36\", direction = \"westbound\")\n\n\nroute_post_eb &lt;- r5r::detailed_itineraries(\n  r5r_network = ttc_prepost_network,\n  origins = humber_point,\n  destinations = finch_west_point,\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2025-12-09 08:00:00\"),\n  shortest_path = FALSE\n) |&gt; \n  mutate(scenario = \"post\", direction = \"eastbound\")\n\nroute_post_wb &lt;- r5r::detailed_itineraries(\n  r5r_network = ttc_prepost_network,\n  origins = finch_west_point,\n  destinations = humber_point,\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2025-12-09 08:00:00\"),\n  shortest_path = FALSE\n) |&gt; \n  mutate(scenario = \"post\", direction = \"westbound\")\n\n\nWith the routes calculated, lets examine the eastbound and westbound travel times pre-construction and now post-construction with Line 6 (Table 1):\n\n\nCode\nroute_pre_eb |&gt; \n  bind_rows(route_pre_wb, route_post_eb, route_post_wb) |&gt; \n  filter(mode != \"WALK\") |&gt; \n  st_drop_geometry() |&gt; \n  select(scenario, direction, mode, route, segment_duration) |&gt; \n  gt::gt()\n\n\n\n\nTable 1: Route-based Travel Times between Humber and Finch West\n\n\n\n\n\n\n\n\n\nscenario\ndirection\nmode\nroute\nsegment_duration\n\n\n\n\npre\neastbound\nBUS\n36\n44.8\n\n\npre\nwestbound\nBUS\n36\n40.3\n\n\npost\neastbound\nTRAM\n6\n46.0\n\n\npost\nwestbound\nTRAM\n6\n46.0\n\n\n\n\n\n\n\n\n\n\nBased on this, it does look like the LRT is scheduled to take 46 minutes end-to-end for this trip, which is a little bit more than the scheduled travel time for the 36 Finch West bus in 2018 (these estimates reflect the in-vehicle time based on scheduled service levels and omit the walking time and wait time that might be required between departing at 8AM and when the first bus arrives).\n\n\nAccessibility Analysis\nAccessibility analysis can give us a better idea of how these travel times might be impacting the potential to reach destinations using the LRT. Let’s examine any differences across the pre-construction and now with the LRT in the network. I will measure accessibility as the number of jobs that can be reached using transit with the same log-logistic impedance function from my previous post. This impedance function is calibrated to commuting flows by transit and places less weight on destinations that are farther away.\nRemember that {r5r} does take service frequency into account in these calculations by running the travel time matrix over a time window and taking the median travel time see vignette. To capture service frequency, I will set this time window to 60 minutes so that we analyze access over the 8:00 AM to 9:00 AM time frame.\n\n\nCode\ntts_jobs &lt;- tts_tazs |&gt; \n  st_drop_geometry() |&gt; \n  select(id, jobs)\n\nlog_logistic_f &lt;- function(t_ij, med_tau, beta) {\n  1 / (1 + (t_ij / med_tau)^beta)\n}\n\naccess_pre &lt;- r5r::travel_time_matrix(\n  r5r_network = ttc_prepost_network,\n  origins = census_data_db_fwlrt |&gt; st_centroid() |&gt; st_transform(crs = 4326),\n  destinations = tts_tazs |&gt; st_centroid() |&gt; st_transform(crs = 4326),\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2018-12-11 08:00:00\"),\n  time_window = 60\n) |&gt;\n  left_join(tts_jobs, by = c(\"to_id\" = \"id\")) |&gt;\n  mutate(weighted_jobs = jobs * log_logistic_f(\n    t_ij = travel_time_p50,\n    med_tau = 49,\n    beta = 4.4856\n  )) |&gt;\n  group_by(from_id) |&gt;\n  summarize(access = sum(weighted_jobs)) |&gt;\n  rename(access_pre = access)\n\naccess_post &lt;- r5r::travel_time_matrix(\n  r5r_network = ttc_prepost_network,\n  origins = census_data_db_fwlrt |&gt; st_centroid() |&gt; st_transform(crs = 4326),\n  destinations = tts_tazs |&gt; st_centroid() |&gt; st_transform(crs = 4326),\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2025-12-09 08:00:00\"),\n  time_window = 60\n) |&gt; \n  left_join(tts_jobs, by = c(\"to_id\" = \"id\")) |&gt;\n  mutate(weighted_jobs = jobs * log_logistic_f(\n    t_ij = travel_time_p50,\n    med_tau = 49,\n    beta = 4.4856\n  )) |&gt;\n  group_by(from_id) |&gt;\n  summarize(access = sum(weighted_jobs)) |&gt;\n  rename(access_post = access)\n\n\nNow join the results back to the DBs and calculate the change in access across the pre- and post-accessibility scenarios:\n\ncensus_data_db_fwlrt_access &lt;- census_data_db_fwlrt |&gt;\n  left_join(access_pre, by = c(\"id\" = \"from_id\")) |&gt;\n  left_join(access_post, by = c(\"id\" = \"from_id\")) |&gt;\n  mutate(access_diff_prepost = ((access_post - access_pre) / access_pre) * 100)\n\nLet’s examine the results changes spatially:\n\n\nCode\ntm_shape(census_data_db, bbox = fwlrt_line_buffer) + tm_polygons(col = \"grey99\", fill = \"grey95\") +\n  tm_shape(census_data_db_fwlrt_access) +\n  tm_fill(\n    fill = \"access_diff_prepost\",\n    fill.scale = tm_scale_intervals(\n      n = 11,\n      style = \"fisher\",\n      midpoint = 0,\n      values = \"-brewer.rd_bu\"\n    ),\n    fill.legend = tm_legend(\n      title = \"accessibility \\nchange (%)\",\n      frame = FALSE #\n    )\n  ) +\n  tm_shape(existing_rt) + tm_lines(col = \"grey35\", lwd = 4) +\n  tm_shape(fwlrt_line) + tm_lines(col = \"grey\", lwd = 4) +\n  tm_shape(fwlrt_stops) + tm_dots(size = .5, fill = \"grey\") +\n  tm_shape(fwlrt_stops) + tm_dots(size = .15, fill = \"white\") \n\n\n\n\n\n\n\n\nFigure 3: Change in accessibility from late 2018 to 2025\n\n\n\n\n\nInteresting! The results in Figure 3 show quite a few DBs gained in access by up to about 17% with the LRT relative to the network in late 2018. However, quite a few DBs also lose access with the percentage change reaching up to about -21%. This is obviously not a great result – the addition of “rapid transit” along the corridor, ostensibly to replace a bus that is often bogged down in traffic, does not stand out as an accessibility “home run” with strictly positive gains in this analysis.\nNevertheless, I will caution that this is comparing the transit networks across 2018 and now late 2025. There could be additional service changes elsewhere in the network that may be contributing to these results, such as slow zones on the subway. Moreover, these are scheduled services and the travel times along particular routes may be adjusted over GTFS releases to better reflect observed travel times."
  },
  {
    "objectID": "posts/slow_transit/index.html#faster-finch",
    "href": "posts/slow_transit/index.html#faster-finch",
    "title": "Slow Transit: Finch West",
    "section": "Faster Finch",
    "text": "Faster Finch\nFor what it is worth, what if the Finch West LRT was operating faster? Again, the milestone testing had the travel time along the line down to about 24 minutes, or an average travel speed of 18kph. How would the accessibility results change if Line 6 was operating at this speed? Faster speeds generally mean more things should be accessible within a given travel time. We can test this by manipulating the GTFS files to make the LRT faster and building a new network for routing.\nFirst, we will create a new network folder (with _ff for “faster Finch”) and copy over the OSM data:\n\nttc_2025_ff_path &lt;- fs::dir_create(\"./r5_networks/ttc_2025_ff\")\n\nfs::file_copy(\n  path = fs::path(data_path, \"openstreetmap_fr_golden_horseshoe-latest.osm.pbf\"),\n  new_path = ttc_2025_ff_path,\n  overwrite = TRUE\n)\n\nOne way to speed up a service is to use the {gtfstools} package, which as limited functions for editing GTFS files. One of these functions is set_trip_speed(), which changes the speeds of the different trips made along a transit route.\nMy note of caution is that this is a very simplified approach limited to changing the end-to-end travel speed of the line and letting the network router interpolate the intermediate stop times. In the original data, stop times are populated along the entire route and reflect some slower speed segments. That original granularity is lost in this approach and some stations are going to overly benefit in this simulation.\nTo change the speeds, we need to read in the late 2025 GTFS files into R and collect all the trips made by Line 6 into a list:\n\nttc_2025_gtfs &lt;- gtfstools::read_gtfs(fs::path(data_path, \"ttc_2025.zip\"))\n\nfwlrt_trip_ids &lt;- ttc_2025_gtfs |&gt; \n  gtfstools::filter_by_route_id(route_id = \"6\") |&gt; \n  pluck(\"trips\") |&gt; \n  select(trip_id) |&gt; \n  deframe()\n\nWe can confirm Line 6’s average speed (in kph) in these GTFS files as:\n\nttc_2025_gtfs |&gt; \n  gtfstools::get_trip_speed(trip_id = fwlrt_trip_ids) |&gt; \n  summarize(speed = mean(speed)) |&gt; \n  pull(speed)\n\n[1] 13.59186\n\n\nLet’s up this to 18kph and write out a new GTFS file to the faster Finch network folder:\n\nttc_2025_ff_gtfs &lt;- ttc_2025_gtfs |&gt;\n  gtfstools::set_trip_speed(trip_id = fwlrt_trip_ids,\n                            speed = 18,\n                            unit = \"km/h\")\n\ngtfstools::write_gtfs(ttc_2025_ff_gtfs,\n                      path = fs::path(ttc_2025_ff_path, \"ttc_2025_ff.zip\"))\n\nHaving vehicles travel faster down the line is going to mess with the vehicle departure and arrival cadence in the scheduled trips, effectively having vehicles travel down the line faster but then wait at the terminus until the next departure time on the old cadence. Thus while the service is faster, more trips are not being made over the 8:00 AM to 9:00 AM time window.\nSolving this would require significantly more effort in editing the GTFS files, likely using sketch planning tools outside of R. But as an illustrative analysis, let’s go with it to see how the faster travel times and service levels affect access. To do this, we will build the network:\n\nttc_2025_ff_network &lt;- r5r::build_network(data_path = ttc_2025_ff_path)\n\nAnd re-run the routing analysis with the faster Finch service on the network:\n\n\nCode\nroute_post_ff_eb &lt;- r5r::detailed_itineraries(\n  r5r_network = ttc_2025_ff_network,\n  origins = humber_point,\n  destinations = finch_west_point,\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2025-12-09 08:00:00\"),\n  shortest_path = FALSE\n) |&gt; \n  mutate(scenario = \"faster finch\", direction = \"eastbound\")\n\nroute_post_ff_wb &lt;- r5r::detailed_itineraries(\n  r5r_network = ttc_2025_ff_network,\n  origins = finch_west_point,\n  destinations = humber_point,\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2025-12-09 08:00:00\"),\n  shortest_path = FALSE\n) |&gt; \n  mutate(scenario = \"faster finch\", direction = \"westbound\")\n\n\nCompared to the previous travel times in Table 1, we can see that the 18kph has dropped the end-to-end travel times to about 35 minutes:\n\n\nCode\nroute_pre_eb |&gt; \n  bind_rows(route_pre_wb, route_post_eb, route_post_wb, route_post_ff_eb, route_post_ff_wb) |&gt; \n  filter(mode != \"WALK\") |&gt; \n  st_drop_geometry() |&gt; \n  select(scenario, direction, mode, segment_duration, route) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n\nscenario\ndirection\nmode\nsegment_duration\nroute\n\n\n\n\npre\neastbound\nBUS\n44.8\n36\n\n\npre\nwestbound\nBUS\n40.3\n36\n\n\npost\neastbound\nTRAM\n46.0\n6\n\n\npost\nwestbound\nTRAM\n46.0\n6\n\n\nfaster finch\neastbound\nTRAM\n34.6\n6\n\n\nfaster finch\nwestbound\nTRAM\n34.6\n6\n\n\n\n\n\n\n\nWhat about changes in accessibility? Let’s re-run the post-scenario accessibility analysis using the faster Finch network:\n\n\nCode\naccess_post_ff &lt;- r5r::travel_time_matrix(\n  r5r_network = ttc_2025_ff_network,\n  origins = census_data_db_fwlrt |&gt; st_centroid() |&gt; st_transform(crs = 4326),\n  destinations = tts_tazs |&gt; st_centroid() |&gt; st_transform(crs = 4326),\n  mode = \"transit\",\n  departure_datetime = ymd_hms(\"2025-12-09 08:00:00\"),\n  time_window = 60\n) |&gt; \n  left_join(tts_jobs, by = c(\"to_id\" = \"id\")) |&gt;\n  mutate(weighted_jobs = jobs * log_logistic_f(\n    t_ij = travel_time_p50,\n    med_tau = 49,\n    beta = 4.4856\n  )) |&gt;\n  group_by(from_id) |&gt;\n  summarize(access = sum(weighted_jobs)) |&gt;\n  rename(access_post_ff = access)\n\n\nAnd map the results:\n\n\nCode\ncensus_data_db_fwlrt_access_ff &lt;- census_data_db_fwlrt_access |&gt; \n  left_join(access_post_ff, by = c(\"id\" = \"from_id\")) |&gt; \n  mutate(\n    access_diff_prepost_ff = ((access_post_ff - access_pre) / access_pre) * 100,\n    access_diff_postpost_ff = ((access_post_ff - access_post) / access_post) * 100\n    ) |&gt; \n  pivot_longer(cols = starts_with(\"access_diff\"), names_to = \"scenario\", values_to = \"access_diff\", names_prefix = \"access_diff_\")\n\ntm_shape(census_data_db, bbox = fwlrt_line_buffer) + tm_polygons(col = \"grey99\", fill = \"grey95\") +\n  tm_shape(census_data_db_fwlrt_access_ff |&gt; filter(scenario != \"postpost_ff\")) +\n  tm_fill(\n    fill = \"access_diff\",\n    fill.free = TRUE,\n    fill.scale = tm_scale_intervals(\n      n = 11,\n      style = \"fisher\",\n      midpoint = 0,\n      values = \"-brewer.rd_bu\"\n    ),\n    fill.legend = tm_legend(\n      title = \"accessibility \\nchange (%)\",\n      frame = FALSE\n    ),\n    fill.chart = tm_chart_histogram()\n  ) +\n  tm_facets_vstack(by = \"scenario\") +\n  tm_shape(existing_rt) + tm_lines(col = \"grey35\", lwd = 4) +\n  tm_shape(fwlrt_line) + tm_lines(col = \"grey\", lwd = 4) +\n  tm_shape(fwlrt_stops) + tm_dots(size = .5, fill = \"grey\") +\n  tm_shape(fwlrt_stops) + tm_dots(size = .15, fill = \"white\") +\n  tm_layout(\n    between.margin = 0.2,\n    panel.label.bg = FALSE,\n    panel.label.frame = FALSE,\n    panel.labels = c(\"original pre-post\", \"pre-post with faster finch\")\n  )\n\n\n\n\n\n\n\n\nFigure 4: Change in accessibility from late 2018 to 2025 by Speed Scenario\n\n\n\n\n\nFrom Figure 4 we can see that the change in speed from 13.6kph to 18kph does help to increase accessibility along the line, with only a small number of DBs still negative.\nFor the current period only, we can see how the increase in speed increases accessibility to jobs by up to about 20% for some DBs:\n\n\nCode\ntm_shape(census_data_db, bbox = fwlrt_line_buffer) + tm_polygons(col = \"grey99\", fill = \"grey95\") +\n  tm_shape(census_data_db_fwlrt_access_ff |&gt; filter(scenario == \"postpost_ff\")) +\n  tm_fill(\n    fill = \"access_diff\",\n\n    fill.scale = tm_scale_intervals(\n      n = 10,\n      style = \"fisher\",\n      midpoint = 0,\n      values = \"-brewer.rd_bu\" \n    ),\n    fill.legend = tm_legend(\n      title = \"accessibility \\nchange (%)\",\n      frame = FALSE \n    )\n  ) +\n  tm_shape(existing_rt) + tm_lines(col = \"grey35\", lwd = 4) +\n  tm_shape(fwlrt_line) + tm_lines(col = \"grey\", lwd = 4) +\n  tm_shape(fwlrt_stops) + tm_dots(size = .5, fill = \"grey\") +\n  tm_shape(fwlrt_stops) + tm_dots(size = .15, fill = \"white\") \n\n\n\n\n\n\n\n\nFigure 5: Change in accessibility across current and sped up Finch West LRT schedules"
  },
  {
    "objectID": "posts/slow_transit/index.html#wrap-up",
    "href": "posts/slow_transit/index.html#wrap-up",
    "title": "Slow Transit: Finch West",
    "section": "Wrap-up",
    "text": "Wrap-up\nI’m still excited about Finch and hopeful that service speeds and frequencies will ramp up as the line finds its groove. While the raw change in average speed used here indirectly penalizes the line for not running more trips, the results still show what we’d expect: speeding up Finch meaningfully increases access.\nBigger picture, Toronto’s surface transit services are often constrained by a transit priority problem – ineffective transit signal priority and a persistent emphasis on preserving car capacity. If we spend billions on an LRT only to have it run like Figure 6, oof. And that’s before we even get to other planning choices like tightly spaced stations (I see you Birchmount–Ionview). Looking forward to testing the “slow transit hypothesis” again if/when the Crosstown opens!\n\n\n\n\n\n\n\n\nFigure 6: Surface Transit in Toronto"
  },
  {
    "objectID": "posts/king_street_priority_corridor/index.html",
    "href": "posts/king_street_priority_corridor/index.html",
    "title": "King Street Transit Priority Corridor Travel Times",
    "section": "",
    "text": "Robert Arku (PhD Student at the University of Toronto) and I are exploring the association between transit accessibility and real estate through an analysis of the King Street Transit Priority Corridor project in Toronto. As part of this, we are estimating the change in streetcar travel times along the corridor using a database of disaggregate streetcar travel times from the TTC and City of Toronto. To coincide with migrating my site over to a Quarto webpage, this seemed like a good topic through which to explore computational blog posts made using RStudio.\nFigure 1: King Street Pilot Project - Planters & furniture near Charlotte St.\nPhoto by TheTrolleyPole via Wikimedia Commons"
  },
  {
    "objectID": "posts/king_street_priority_corridor/index.html#king-street-transit-priority-corridor",
    "href": "posts/king_street_priority_corridor/index.html#king-street-transit-priority-corridor",
    "title": "King Street Transit Priority Corridor Travel Times",
    "section": "King Street Transit Priority Corridor",
    "text": "King Street Transit Priority Corridor\nThe Transit Priority corridor launched as a pilot project on November 12, 2017, with the primary objective of improving transit service along a 2.6km stretch of King Street between Bathurst and Jarvis Streets in downtown Toronto (Figure 2).\n\n\n\n\n\n\n\n\nFigure 2: King Street Pilot Corridor\n\n\n\n\n\nImage courtesy of the City of Toronto\nTransit priority was achieved by restricting through-movements and left turns for private vehicles, removing on-street parking, and giving priority transit vehicles (Figure 3). Streetscape improvements such as seating, patios, public art, bicycle parking, and parkettes made the corridor more pedestrian- and cyclist-friendly.\n\n\n\n\n\n\n\n\nFigure 3: Sample Urban Design Plan for the King Street Pilot Corridor Project\n\n\n\n\n\nImage courtesy of the City of Toronto\nAs a pilot project, the City of Toronto collected a wealth of data related to transit vehicle performance, traffic, pedestrian and cyclist volumes, business performance, and stakeholder feedback along the corridor and posted regular updates through a project dashboard. One of these datasets is streetcar travel times collected using the TTC’s Communication and Information System (CIS) that tracks transit vehicle movements throughout the city.\nWith regards to primary objective of increasing travel speeds and the reliability of streetcar services along the corridor, previous analyses conducted by the City and others (e.g. 1; 2) have examined CIS and GPS data and concluded that the implementation of the priority corridor generally resulted in decreases in travel times and improvements to service reliability. We will follow their lead to calculate the change in travel times across the pre-pilot and pilot period phases from the CIS data."
  },
  {
    "objectID": "posts/king_street_priority_corridor/index.html#load-ttc-cis-data",
    "href": "posts/king_street_priority_corridor/index.html#load-ttc-cis-data",
    "title": "King Street Transit Priority Corridor Travel Times",
    "section": "Load TTC CIS Data",
    "text": "Load TTC CIS Data\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(gt)\nlibrary(ggdist)\nlibrary(opendatatoronto)\n\nThe first step is to use the {opendatatoronto} package to fetch the resources associated with the disaggregate streetcar headway and travel time data package:\n\npackage_resources &lt;- list_package_resources(\"e74ba0ba-07c8-4a18-bac9-57bd2b9698c1\")\npackage_resources %&gt;% glimpse()\n\nRows: 3\nColumns: 4\n$ name          &lt;chr&gt; \"ttc-king-st-pilot-disaggregate-weekday-travel-time-2017…\n$ id            &lt;chr&gt; \"dbe81d22-d5a4-49db-a454-aad69e1e0140\", \"5df19475-bc6c-4…\n$ format        &lt;chr&gt; \"XLSX\", \"XLSX\", \"CSV\"\n$ last_modified &lt;date&gt; 2019-07-23, 2019-07-23, 2019-07-23\n\n\nThere are three data files within this package: travel times and headways in .xlsx format and a summary .csv file. Let’s get the travel time resource using its resource ID and take a glimpse:\n\nresource_id &lt;- package_resources %&gt;%\n  filter(name == \"ttc-king-st-pilot-disaggregate-weekday-travel-time-2017-2018-xlsx\") %&gt;%\n  pull(id)\n\nresource &lt;- get_resource(resource_id)\nresource %&gt;% glimpse()\n\nList of 3\n $ Sheet1: tibble [342,759 × 14] (S3: tbl_df/tbl/data.frame)\n  ..$ TripID       : num [1:342759] 16053362 16053365 16053215 16052346 16052017 ...\n  ..$ RouteNumber  : chr [1:342759] \"304\" \"304\" \"504\" \"504\" ...\n  ..$ VehicleNumber: chr [1:342759] \"4179\" \"4179\" \"4157\" \"4076\" ...\n  ..$ RunNumber    : num [1:342759] 15 15 1 15 35 30 19 31 7 21 ...\n  ..$ Direction    : chr [1:342759] \"EAST\" \"WEST\" \"WEST\" \"EAST\" ...\n  ..$ FromStopAbbr : chr [1:342759] \"BKIN\" \"KGJV\" \"KGJV\" \"BKIN\" ...\n  ..$ FromStopName : chr [1:342759] \"KING ST WEST AT BATHURST ST\" \"KING ST EAST AT JARVIS ST\" \"KING ST EAST AT JARVIS ST\" \"KING ST WEST AT BATHURST ST\" ...\n  ..$ ToStopAbbr   : chr [1:342759] \"KGJV\" \"BKIN\" \"BKIN\" \"KGJV\" ...\n  ..$ ToStopName   : chr [1:342759] \"KING ST EAST AT JARVIS ST\" \"KING ST WEST AT BATHURST ST\" \"KING ST WEST AT BATHURST ST\" \"KING ST EAST AT JARVIS ST\" ...\n  ..$ TimePeriod   : chr [1:342759] \"0-EARLY (3am-7am)\" \"0-EARLY (3am-7am)\" \"0-EARLY (3am-7am)\" \"0-EARLY (3am-7am)\" ...\n  ..$ ObservedDate : chr [1:342759] \"2017-01-03\" \"2017-01-03\" \"2017-01-03\" \"2017-01-03\" ...\n  ..$ TripTime     : POSIXct[1:342759], format: \"2017-01-03 03:43:00\" \"2017-01-03 04:42:40\" ...\n  ..$ RunningTime  : num [1:342759] 14.33 10 9.67 12.67 12.33 ...\n  ..$ Speed        : num [1:342759] 10.8 15.2 15.5 11.7 12.2 ...\n $ Sheet2: tibble [0 × 0] (S3: tbl_df/tbl/data.frame)\n Named list()\n $ Sheet3: tibble [0 × 0] (S3: tbl_df/tbl/data.frame)\n Named list()\n\n\nWhile the {opendatatoronto} package downloads the data in an R-friendly format, because the original streetcar travel times file was a .xlsx, it looks like the returned data resource is actually a list of three tibbles structured around Excel sheets (e.g. Sheet1, Sheet2, Sheet3). The streetcar travel times data is in Sheet1, so next we will use pluck from the {purrr} package to extract the trip times:\n\ntrip_times &lt;- resource %&gt;% pluck(\"Sheet1\")"
  },
  {
    "objectID": "posts/king_street_priority_corridor/index.html#prepare-data",
    "href": "posts/king_street_priority_corridor/index.html#prepare-data",
    "title": "King Street Transit Priority Corridor Travel Times",
    "section": "Prepare Data",
    "text": "Prepare Data\nThe CIS data contains travel times and speeds for eastbound and westbound streetcar trips across the Pilot corridor along King Street between stops at Bathurst and Jarvis Streets. The trips are grouped into major service time periods (e.g. AM Peak, PM Peak, etc.) and the data also includes general information such as the streetcar route number and vehicle and trip IDs. To prepare the travel times for further analysis, we will do some data operations such as converting the ObservedDate from a character type to a proper date, getting the day of the week, and some factor operations including creating a variable corresponding to the baseline and pilot time periods:\n\ntrip_times &lt;- trip_times %&gt;%\n  mutate(\n    # make into proper datetime\n    ObservedDate = as_date(ObservedDate),\n    \n    # get day of week to make sure no weekends\n    day_name = wday(ObservedDate, label = TRUE),\n    \n    TimePeriod = str_sub(TimePeriod, start = 3),\n    TimePeriod = as.factor(TimePeriod),\n    TimePeriod = fct_relevel(TimePeriod, \n                             c(\"EARLY (3am-7am)\",\n                               \"AM (7am-10am)\",\n                               \"MID (10am-4pm)\",\n                               \"PM (4pm-7pm)\",\n                               \"EVENING (7pm-10pm)\",\n                               \"LATE (10pm-)\")),\n    \n    # classify baseline and pilot time periods\n    phase = case_when(ObservedDate &gt; as_date(\"2017-11-12\") ~ \"pilot\",\n                     TRUE ~ \"baseline\"),\n    phase = as.factor(phase),\n    phase = fct_relevel(phase, c(\"baseline\", \"pilot\")),\n    \n    # recode direction\n    Direction = fct_recode(Direction, eastbound = \"EAST\", westbound = \"WEST\"))\n\nThe final step is to filter the trip times data down to trips that occurred between the start of September 2017 and the end of January 2018. This corresponds to about 2.5 months of data on either side of the pilot implementation on November 12, 2017.\n\nfiltered_trip_times &lt;- trip_times %&gt;%\n  filter(between(x = ObservedDate, \n                 left = as_date(\"2017-09-04\"), \n                 right = as_date(\"2018-01-31\")), \n         day_name != \"Sat\")"
  },
  {
    "objectID": "posts/king_street_priority_corridor/index.html#results",
    "href": "posts/king_street_priority_corridor/index.html#results",
    "title": "King Street Transit Priority Corridor Travel Times",
    "section": "Results",
    "text": "Results\nFigure 4 shows the final plot of travel times before and after the introduction of the pilot. It certainly looks as though the implementation of the pilot reduced average travel times and decreased travel time variability within this time window.\n\nggplot(filtered_trip_times, aes(x = ObservedDate, y = RunningTime)) + \n  geom_point(position = position_jitter(seed = 1, width = 0.2), \n             size = 0.2, col = \"grey80\", alpha = 0.5) +\n  geom_smooth(se = FALSE, method = \"gam\", \n              formula = y ~ s(log(x)), aes(colour = phase)) +\n  facet_grid(rows = vars(Direction)) + \n  geom_vline(aes(xintercept = lubridate::as_date(\"2017-11-12\")), \n             linetype = \"dashed\", size = 0.25) +\n  ylim(c(0, 80)) +\n  ylab(\"minutes\") +\n  #ggtitle(\"Priority Corridor Travel Times\") +\n  scale_colour_manual(values = c(\"#33638DFF\", \"#3CBB75FF\")) +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        legend.position=\"bottom\")\n\n\n\n\n\n\n\nFigure 4: Priority Corridor Travel Times: Baseline and Pilot\n\n\n\n\n\nThis is generally confirmed by calculating the mean travel times in the eastbound and westbound directions across phases (Table 1). Here we see a decrease in travel times across every time period other than the 3am-7am Early service period, with the biggest difference in the PM peak.\n\nfiltered_trip_times %&gt;%\n  group_by(phase, TimePeriod, Direction) %&gt;%\n  summarize(mean_tt = mean(RunningTime)) %&gt;%\n  \n  # double pivot to get travel times by direction and phase\n  pivot_wider(names_from = Direction, values_from = mean_tt) %&gt;%\n  pivot_wider(names_from = phase, values_from = c(\"eastbound\", \"westbound\")) %&gt;%\n  ungroup() %&gt;%\n  \n  # calculate travel time differences and relocate eastbound result for table\n  mutate(eastbound_difference = eastbound_pilot - eastbound_baseline,\n         westbound_difference = westbound_pilot - westbound_baseline) %&gt;%\n  relocate(eastbound_difference, .after = eastbound_pilot) %&gt;%\n  \n  # create a gt table\n  gt(rowname_col = \"TimePeriod\") %&gt;%\n  \n  # create table grouping based on direction - split at delimiter e.g. eastbound_pilot\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  \n  # format table numbers\n  fmt_number(contains(c(\"baseline\", \"pilot\", \"difference\")), decimals = 1)\n\n\n\nTable 1: Priority Corridor Average Travel Times: Baseline and Pilot (minutes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neastbound\n\n\nwestbound\n\n\n\nbaseline\npilot\ndifference\nbaseline\npilot\ndifference\n\n\n\n\nEARLY (3am-7am)\n12.9\n13.0\n0.1\n12.4\n12.4\n0.1\n\n\nAM (7am-10am)\n16.3\n15.2\n−1.1\n15.8\n14.5\n−1.3\n\n\nMID (10am-4pm)\n17.8\n14.5\n−3.2\n17.2\n13.8\n−3.4\n\n\nPM (4pm-7pm)\n21.3\n17.0\n−4.2\n21.2\n16.5\n−4.7\n\n\nEVENING (7pm-10pm)\n17.3\n13.8\n−3.5\n18.1\n13.9\n−4.2\n\n\nLATE (10pm-)\n15.8\n12.8\n−3.0\n15.4\n12.1\n−3.2\n\n\n\n\n\n\n\n\n\n\nWhile some special events in the fall of 2017 impact the travel times in this baseline period, such as the Toronto International Film Festival, doing the same analysis to compare the baseline period with data from one year later (September to end of October 2018) shows some stability in the reductions in average travel times over the service periods (Figure 5).\n\nfiltered_trip_times_late_2018 &lt;- trip_times %&gt;%\n  filter(between(x = ObservedDate, \n                 left = as_date(\"2018-09-01\"), \n                 right = as_date(\"2018-10-31\")), \n         day_name != \"Sat\") %&gt;%\n  mutate(phase = \"late 2018\") %&gt;%\n  bind_rows(filtered_trip_times) %&gt;%\n  mutate(phase = as.factor(phase),\n         phase = fct_relevel(phase,\n                             \"baseline\", \"pilot\", \"late 2018\"))\n\n\nggplot(filtered_trip_times_late_2018, \n       aes(y = RunningTime, x = TimePeriod, fill = phase)) +\n  stat_slab(side = \"both\", \n            scale = 0.5,\n            show.legend = TRUE,\n            position = position_dodge(width = .8),\n            aes(fill_ramp = stat(level)),\n            .width = c(.50, .75, .95, 1)) +\n  stat_summary(fun = \"mean\",\n               geom = \"crossbar\", \n               width = 0.5,\n               lwd = 0.1,\n               position = position_dodge(width = .8),\n               show.legend = F)  +\n  scale_colour_manual(values = c(\"#33638DFF\", \"#3CBB75FF\", \"#FDE725FF\"), aesthetics = \"fill\")+\n  guides(fill_ramp = \"none\") +\n  ylab(\"minutes\") +\n  ylim(10, 30) +\n  facet_grid(rows = vars(as.factor(Direction))) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n        legend.position=\"top\")\n\n\n\n\n\n\n\nFigure 5: Priority Corridor Travel Times: Baseline, Pilot, and Late 2018\n\n\n\n\n\nThe results from this analysis are feeding into our ongoing work looking into how changes in travel times contribute to changes in accessibility and real estate prices - more on this soon!"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Riaz, S., Higgins, C., & Farber, S. (2025). Essential Workers and Transit Ridership Retention During COVID-19 Shocks in the Toronto Region. Findings. https://doi.org/10.32866/001c.141220\n\n\nHiggins, C. D., Murakami, J., & He, S. Y. (2025). Accessibility and property values in hong kong (pp. 358–379). Edward Elgar Publishing. https://doi.org/10.4337/9781035309245.00025\n\n\nHiggins, C. D., Arku, R. N., Farber, S., & Miller, E. J. (2025). Multimodal Accessibility and the Capitalization of Realized Access by Car and Transit in Housing Prices Across Canada. Annals of the American Association of Geographers, 1–26. https://doi.org/10.1080/24694452.2025.2495075\n\n\nSoukhov, A., Higgins, C. D., Páez, A., & Mohamed, M. (2025). Ten Years of School Closures and Consolidations in Hamilton, Canada and the Impact on Multimodal Accessibility. Networks and Spatial Economics. https://doi.org/10.1007/s11067-025-09677-z\n\n\nArku, R. N., Higgins, C. D., Fischer, J., & Farber, S. (2024). Do affluent neighbourhoods pay more for transit access? Exploring the capitalization of employment accessibility within different housing submarkets in Vancouver. Journal of Transport Geography, 121, 104038. https://doi.org/10.1016/j.jtrangeo.2024.104038\n\n\nHiggins, C. D., Arku, R. N., Farber, S., & Miller, E. J. (2024). Modelling changes in accessibility and property values associated with the King Street Transit Priority Corridor project in Toronto. Transportation Research Part A: Policy and Practice, 190, 104256. https://doi.org/10.1016/j.tra.2024.104256\n\n\nParga, J. P. F. A., Tiznado-Aitken, I., Jamal, S., Farber, S., Yu, A., & Higgins, C. (2024). Perceived accessibility and self-rated health: Examining subjective well-being in the suburbs of Scarborough, Canada. Transportation Research Part A: Policy and Practice, 190, 104261. https://doi.org/10.1016/j.tra.2024.104261\n\n\nNg, K. Y., Hong, A., Higgins, C. D., Widener, M. J., & Koh, K. (2024). Beyond distance: Measuring spatial accessibility to healthy food for older adults in Hong Kong using a 3D least-effort method. Applied Geography, 169, 103336. https://doi.org/10.1016/j.apgeog.2024.103336\n\n\nTabascio, A., Tiznado-Aitken, I., Higgins, C., & Farber, S. (2024). Incorporating equity into transit performance measures: A disaggregated bus route level approach. Case Studies on Transport Policy, 101256. https://doi.org/10.1016/j.cstp.2024.101256\n\n\nYu, A., & Higgins, C. D. (2024). Travel behaviour and the 15-min City: Access intensity, sufficiency, and non-work car use in Toronto. Travel Behaviour and Society, 36, 100786. https://doi.org/10.1016/j.tbs.2024.100786\n\n\nMerrall, J., Higgins, C. D., & Paez, A. (2023). What’s a School Worth to a Neighborhood? A Spatial Hedonic Analysis of Property Prices in the Context of Accommodation Reviews in Ontario. Geographical Analysis. https://doi.org/10.1111/gean.12377\n\n\nAllen, J., Higgins, C. D., Silver, D., & Farber, S. (2023). Are low-income residents disproportionately moving away from transit? Journal of Transport Geography, 110, 103635. https://doi.org/10.1016/j.jtrangeo.2023.103635\n\n\nShiv Gazi, Y., Higgins, C. D., Kumar, G., & Palm, M. (2023). Public Transport Access to Drug Treatment Before and During COVID-19: Implications for the Opioid Epidemic. International Journal of Drug Policy, 104032. https://doi.org/10.1016/j.drugpo.2023.104032\n\n\nSoukhov, A., Páez, A., Higgins, C. D., & Mohamed, M. (2023). Introducing spatial availability, a singly-constrained measure of competitive accessibility. PLOS ONE, 18(1), e0278468. https://doi.org/10.1371/journal.pone.0278468\n\n\nDemitiry, M., Higgins, C. D., Páez, A., & Miller, E. J. (2022). Accessibility to primary care physicians: Comparing floating catchments with a utility-based approach. Journal of Transport Geography, 101, 103356. https://doi.org/10.1016/j.jtrangeo.2022.103356\n\n\nHiggins, C. D., Palm, M., DeJohn, A., Xi, L., Vaughan, J., Farber, S., Widener, M., & Miller, E. (2022). Calculating place-based transit accessibility: Methods, tools and algorithmic dependence. Journal of Transport and Land Use, 15(1). https://doi.org/10.5198/jtlu.2022.2012\n\n\nDesjardins, E., Higgins, C. D., & Páez, A. (2022). Examining equity in accessibility to bike share: A balanced floating catchment area approach. Transportation Research Part D: Transport and Environment, 102, 103091. https://doi.org/10.1016/j.trd.2021.103091\n\n\nHiggins, C. D., Páez, A., Kim, G., & Wang, J. (2021). Changes in accessibility to emergency and community food services during COVID-19 and implications for low income populations in Hamilton, Ontario. Social Science & Medicine, 291, 114442. https://doi.org/10.1016/j.socscimed.2021.114442\n\n\nHiggins, C. D. (2021). Hiking with Tobler: Tracking Movement and Calibrating a Cost Function for Personalized 3D Accessibility. Findings. https://doi.org/10.32866/001c.28107\n\n\nDesjardins, E., Higgins, C. D., Scott, D. M., Apatu, E., & Páez, A. (2021). Using environmental audits and photo-journeys to compare objective attributes and bicyclists’ perceptions of bicycle routes. Journal of Transport & Health, 22, 101092. https://doi.org/10.1016/j.jth.2021.101092\n\n\nDesjardins, E., Apatu, E., Razavi, S. D., Higgins, C. D., Scott, D. M., & Páez, A. (2021). “Going through a little bit of growing pains”: A qualitative study of the factors that influence the route choice of regular bicyclists in a developing cycling city. Transportation Research Part F: Traffic Psychology and Behaviour, 81, 431–444. https://doi.org/10.1016/j.trf.2021.06.005\n\n\nDesjardins, E., Higgins, C. D., Scott, D. M., Apatu, E., & Páez, A. (2021). Correlates of bicycling trip flows in Hamilton, Ontario: fastest, quietest, or balanced routes? Transportation, 49(3), 867–895. https://doi.org/10.1007/s11116-021-10197-1\n\n\nPáez, A., & Higgins, C. D. (2021). The Accessibility Implications of a Pilot COVID-19 Vaccination Program in Hamilton, Ontario. Findings. https://doi.org/10.32866/001c.24082\n\n\nBruyns, G. J., Higgins, C. D., & Nel, D. H. (2020). Urban volumetrics: From vertical to volumetric urbanisation and its extensions to empirical morphological analysis. Urban Studies, 58(5), 922–940. https://doi.org/10.1177/0042098020936970\n\n\nPáez, A., Anjum, Z., Dickson-Anderson, S. E., Schuster-Wallace, C. J., Martín Ramos, B., & Higgins, C. D. (2020). Comparing distance, time, and metabolic energy cost functions for walking accessibility in infrastructure-poor regions. Journal of Transport Geography, 82, 102564. https://doi.org/10.1016/j.jtrangeo.2019.102564\n\n\nPáez, A., Higgins, C. D., & Vivona, S. F. (2019). Demand and level of service inflation in Floating Catchment Area (FCA) methods. PLOS ONE, 14(6), e0218773. https://doi.org/10.1371/journal.pone.0218773\n\n\nHiggins, C. D. (2019). Accessibility toolbox for r and ArcGIS. Transport Findings. https://doi.org/10.32866/8416\n\n\nHiggins, C. D. (2019). A 4D spatio-temporal approach to modelling land value uplift from rapid transit in high density and topographically-rich cities. Landscape and Urban Planning, 185, 68–82. https://doi.org/10.1016/j.landurbplan.2018.12.011\n\n\nHiggins, C. D., Adams, M. D., Réquia, W. J., & Mohamed, M. (2019). Accessibility, air pollution, and congestion: Capturing spatial trade-offs from agglomeration in the property market. Land Use Policy, 84, 177–191. https://doi.org/10.1016/j.landusepol.2019.03.002\n\n\nMohamed, M., Higgins, C. D., Ferguson, M., & Réquia, W. J. (2018). The influence of vehicle body type in shaping behavioural intention to acquire electric vehicles: A multi-group structural equation approach. Transportation Research Part A: Policy and Practice, 116, 54–72. https://doi.org/10.1016/j.tra.2018.05.011\n\n\nRequia, W. J., Mohamed, M., Higgins, C. D., Arain, A., & Ferguson, M. (2018). How clean are electric vehicles? Evidence-based review of the effects of electric mobility on air pollutants, greenhouse gas emissions and human health. Atmospheric Environment, 185, 64–77. https://doi.org/10.1016/j.atmosenv.2018.04.040\n\n\nRequia, W. J., Higgins, C. D., Adams, M. D., Mohamed, M., & Koutrakis, P. (2018). The health impacts of weekday traffic: A health risk assessment of PM2.5 emissions during congested periods. Environment International, 111, 164–176. https://doi.org/10.1016/j.envint.2017.11.025\n\n\nFerguson, M., Mohamed, M., Higgins, C. D., Abotalebi, E., & Kanaroglou, P. (2018). How open are Canadian households to electric vehicles? A national latent class choice analysis with willingness-to-pay and metropolitan characterization. Transportation Research Part D: Transport and Environment, 58, 208–224. https://doi.org/10.1016/j.trd.2017.12.006\n\n\nHiggins, C., & Kanaroglou, P. (2017). Rapid transit, transit-oriented development, and the contextual sensitivity of land value uplift in Toronto. Urban Studies, 55(10), 2197–2225. https://doi.org/10.1177/0042098017712680\n\n\nHiggins, C. D., Mohamed, M., & Ferguson, M. R. (2017). Size matters: How vehicle body type affects consumer preferences for electric vehicles. Transportation Research Part A: Policy and Practice, 100, 182–201. https://doi.org/10.1016/j.tra.2017.04.014\n\n\nHiggins, C. D., Sweet, M. N., & Kanaroglou, P. S. (2017). All minutes are not equal: travel time and the effects of congestion on commute satisfaction in Canadian cities. Transportation, 45(5), 1249–1268. https://doi.org/10.1007/s11116-017-9766-2\n\n\nMohamed, M., Higgins, C. D., Ferguson, M., & Kanaroglou, P. (2016). Identifying and characterizing potential electric vehicle adopters in Canada: A two-stage modelling approach. Transport Policy, 52, 100–112. https://doi.org/10.1016/j.tranpol.2016.07.006\n\n\nHiggins, C. D., & Kanaroglou, P. S. (2016). Infrastructure or Attraction? Image-led Planning and the Intangible Objectives of Rapid Transit Projects. Journal of Planning Literature, 31(4), 452–462. https://doi.org/10.1177/0885412216667899\n\n\nHiggins, C. D., & Kanaroglou, P. S. (2016). Forty years of modelling rapid transit’s land value uplift in North America: moving beyond the tip of the iceberg. Transport Reviews, 36(5), 610–634. https://doi.org/10.1080/01441647.2016.1174748\n\n\nHiggins, C. D., & Kanaroglou, P. S. (2016). A latent class method for classifying and evaluating the performance of station area transit-oriented development in the Toronto region. Journal of Transport Geography, 52, 61–72. https://doi.org/10.1016/j.jtrangeo.2016.02.012\n\n\nKanaroglou, P. S., Higgins, C. D., & Chowdhury, T. A. (2015). Excess commuting: a critical review and comparative analysis of concepts, indices, and policy implications. Journal of Transport Geography, 44, 13–23. https://doi.org/10.1016/j.jtrangeo.2015.02.009\n\n\nHiggins, C. D., Ferguson, M., & Kanaroglou, P. (2014). Light rail and land use change: Rail transit’s role in reshaping and revitalizing cities. Journal of Public Transportation, 17(2), 93–112. https://doi.org/10.5038/2375-0901.17.2.5\n\n\nHiggins, C. D., & Huque, A. S. (2014). Public Money and Mickey Mouse: Evaluating performance and accountability in the Hong Kong Disneyland joint venture publicprivate partnership. Public Management Review, 17(8), 1103–1123. https://doi.org/10.1080/14719037.2014.881533\n\n\nHiggins, C. D., Ferguson, M., & Kanaroglou, P. S. (2012). Varieties of Logistics Centers. Transportation Research Record: Journal of the Transportation Research Board, 2288(1), 9–18. https://doi.org/10.3141/2288-02"
  }
]